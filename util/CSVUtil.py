# !/usr/bin/env python3
# -*- coding:utf-8 -*-

import csv
import re
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Optional, List, Type, TypeVar, Union, Dict, Callable

import numpy as np
import pandas as pd

from util.CommonUtil import CommonUtil
from util.FileUtil import FileUtil

T = TypeVar('T')  # æ³›å‹ç±»å‹

"""
CSV/padanså·¥å…·ç±»
æ”¯æŒ: 
* read_csv(): è¯»å–csvæ–‡ä»¶, å¹¶å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²

* read_csv_to_objects(): è¯»å–csvæ–‡ä»¶, å¯¹è¡Œæ•°æ®è¿›è¡Œå­—ç¬¦æ›¿æ¢, å¹¶å°†æ¯è¡Œæ•°æ®è½¬åŒ–ä¸ºç‰¹å®šçš„å¯¹è±¡
* extract_csv(): ä»csvæ–‡ä»¶ä¸­æå–æŒ‡å®šåˆ—çš„æ•°æ®, æ”¯æŒå…³é”®å­—å’Œè¡Œå·è¿‡æ»¤, å¹¶è¿›è¡Œæ•°æ®æ¸…æ´—å’Œå¤„ç†

* merge_csv(): åˆå¹¶å¤šä¸ªcsvæ–‡ä»¶, æ”¯æŒæŒ‰åˆ—ååˆå¹¶, å¹¶è¿›è¡Œæ•°æ®å»é‡
* merge_dataframe(): åˆå¹¶å¤šä¸ªDataFrame, æ”¯æŒæŒ‰åˆ—ååˆå¹¶, å¹¶è¿›è¡Œæ•°æ®å»é‡

* calc_csv_accuracy(): è®¡ç®—csvæ–‡ä»¶ä¸­æŒ‡å®šåˆ—çš„å‡†ç¡®ç‡(åŸºäºæŒ‡å®šçš„çœŸå€¼åŸºå‡†åˆ—, å»ç»Ÿè®¡æŒ‡å®šåˆ—çš„å‡†ç¡®ç‡)
* calc_dataframe_accuracy(): è®¡ç®—DataFrameä¸­æŒ‡å®šåˆ—çš„å‡†ç¡®ç‡

* to_markdown(): å°†DataFrameè½¬æ¢ä¸ºMarkdownæ ¼å¼çš„å­—ç¬¦ä¸², å¹¶å­˜å‚¨åˆ°æŒ‡å®šçš„æ–‡ä»¶ä¸­

* filter_and_replace_dataframe(): è¿‡æ»¤æŒ‡å®šåˆ—çš„æ•°æ®, å¯¹ç»“æœè¿›è¡Œè¡Œå·äºŒæ¬¡è¿‡æ»¤å’ŒæŒ‡å®šæ•°æ®æ›¿æ¢

å…¶ä»–å¸¸ç”¨api:
é‡å‘½ååˆ—å: df = df.rename(columns={'a': 'b'})
æ‹¼æ¥ä¸åŒçš„df:  df = pd.concat([df1, df2, df3], ignore_index=True)
"""


class CSVUtil(object):

    @staticmethod
    def read_csv(src_path: str, usecols: Optional[Union[pd.Index, List[str]]] = None, skip_rows: int = 0, encoding: str = 'utf-8-sig') -> pd.DataFrame:
        """
        ä»¥stræ ¼å¼è¯»å–CSVæ–‡ä»¶, å¹¶å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        :param src_path: csvæºæ–‡ä»¶è·¯å¾„
        :param encoding: ç¼–ç 
        :param usecols: è¦è¯»å–çš„åˆ—, Noneæˆ–[] è¡¨ç¤ºè¯»å–å…¨éƒ¨åˆ—,å¦åˆ™åªä¼šä¿ç•™æœ‰å®šä¹‰çš„åˆ—(è‹¥åˆ—ä¸å­˜åœ¨, ä¼šè‡ªåŠ¨æ·»åŠ )
        :param skip_rows: è¦è·³è¿‡è¯»å–çš„è¡Œæ•°
        """
        try:
            df = pd.read_csv(src_path, encoding=encoding, dtype=str, skiprows=skip_rows, on_bad_lines='skip')
        except UnicodeDecodeError:
            encoding = FileUtil.detect_encoding(src_path, 'utf-8-sig')
            df = pd.read_csv(src_path, encoding=encoding, dtype=str, skiprows=skip_rows)

        df = CSVUtil.reorder_cols(df, usecols)
        return df.fillna('')

    @staticmethod
    def reorder_cols(df: pd.DataFrame, usecols: Optional[Union[pd.Index, List[str]]] = None) -> pd.DataFrame:
        """
        é‡æ’å¹¶åªä¿ç•™æŒ‡å®šçš„åˆ—æ•°æ®
        è‹¥è¦ä¿®æ”¹åˆ—åè¯·è‡ªè¡Œè°ƒç”¨æ¥å£: df=df.rename({'a':'b'}, inplace=False)
        æ³¨æ„è¿”å›çš„æ–°çš„df ä¸å½±å“å…¥å‚çš„æºdf, è¯·æŒ‰éœ€é‡æ–°èµ‹å€¼
        """
        df = CSVUtil.add_cols(df, usecols)
        if df is not None and not CommonUtil.isNoneOrBlank(usecols):
            df = df[usecols]  # é‡æ’é¡ºåº
        return df

    @staticmethod
    def add_cols(df: pd.DataFrame, usecols: Optional[Union[pd.Index, List[str]]] = None) -> Optional[pd.DataFrame]:
        """
        æŒ‰éœ€æ·»åŠ åˆ—
        è‹¥è¦ä¿®æ”¹åˆ—åè¯·è‡ªè¡Œè°ƒç”¨æ¥å£: df=df.rename({'a':'b'}, inplace=False)
        """
        if df is None:
            return None

        if not CommonUtil.isNoneOrBlank(usecols):
            for col in usecols:
                if col not in df.columns:
                    df[col] = ''  # åˆå§‹åŒ–ä¸ºç©ºå­—ç¬¦
                    CommonUtil.printLog(f'{col}åˆ—ä¸å­˜åœ¨, æ·»åŠ ')
        return df

    @staticmethod
    def contains_cols(df: pd.DataFrame, cols: List[str], all_match: bool = True) -> bool:
        """
        æ£€æŸ¥DataFrameä¸­æ˜¯å¦åŒ…å«æŒ‡å®šçš„åˆ—
        :param df: DataFrame
        :param cols: åˆ—ååˆ—è¡¨
        :param all_match: æ˜¯å¦å…¨éƒ¨åŒ¹é…, True-å…¨éƒ¨åŒ¹é…, False-åªè¦æœ‰ä¸€ä¸ªåŒ¹é…å³å¯
        :return: æ˜¯å¦åŒ…å«
        """
        if df is None or CommonUtil.isNoneOrBlank(cols):
            return False
        columns = df.columns.tolist()
        match_result = [col in columns for col in cols]
        return all(match_result) if all_match else any(match_result)

    @staticmethod
    def to_csv(df: pd.DataFrame, output_path: str, encoding: str = 'utf-8-sig', index=False, lineterminator='\n', mode: str = 'w') -> bool:
        """
        å°†DataFrameä¿å­˜ä¸ºCSVæ–‡ä»¶
        :param df: DataFrame
        :param output_path: è¾“å‡ºè·¯å¾„
        :param encoding: ç¼–ç 
        :param index: æ˜¯å¦ä¿å­˜ç´¢å¼•
        :param lineterminator: è¡Œåˆ†éš”ç¬¦
        :param mode: ä¿å­˜æ¨¡å¼, w-è¦†ç›–  a-è¿½åŠ 
        """
        if CommonUtil.isNoneOrBlank(output_path):
            return False

        try:
            FileUtil.createFile(output_path, False)
            df.to_csv(output_path, index=index, encoding=encoding, lineterminator=lineterminator, mode=mode)
            CommonUtil.printLog(f'to_csv success: total rows={len(df)}, ä¿å­˜æ•°æ®åˆ°: {output_path}')
            return True
        except Exception as e:
            CommonUtil.printLog(f'to_csv fail: {e}\nä¿å­˜æ•°æ®åˆ°: {output_path}')
            return False

    @staticmethod
    def read_csv_to_objects(
            file_path: str,
            object_class: Type[T],
            skip_rows: int = 0,
            delimiter: str = ',',
            encoding: str = 'utf-8',
            skip_empty_line: bool = True,
            replace_dict: Optional[Dict[str, str]] = None
    ) -> List[T]:
        """
        è¯»å–CSVæ–‡ä»¶å¹¶è½¬æ¢ä¸ºå¯¹è±¡åˆ—è¡¨
        ä¼šè·³è¿‡ä»¥ # æˆ– ; å¼€å¤´çš„è¡Œ
        skip_empty_line=Trueæ—¶, è¿˜ä¼šè·³è¿‡ç©ºç™½è¡Œ
        æ³›å‹å¯¹è±¡Tä¸­å¿…é¡»åŒ…å«æœ‰ä¸€ä¸ªå‡½æ•°:

        @classmethod
        def by_csv_row(cls, row: List[str]):
            pass

        å‚æ•°:
        - file_path: CSVæ–‡ä»¶è·¯å¾„
        - object_class: ç›®æ ‡å¯¹è±¡ç±»ï¼ˆéœ€å®ç° by_csv_row æ–¹æ³•ï¼‰
        - skip_rows: è·³è¿‡çš„è¡Œæ•°ï¼ˆé»˜è®¤ä¸º1ï¼Œè·³è¿‡æ ‡é¢˜è¡Œï¼‰
        - delimiter: åˆ†éš”ç¬¦ï¼ˆé»˜è®¤ä¸ºé€—å·ï¼‰
        - encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ä¸ºutf-8ï¼‰
        - skip_empty_line: æ˜¯å¦è·³è¿‡ç©ºè¡Œ
        - replace_dict: å°†æŒ‡å®šçš„å­—ç¬¦ä¸²æ›¿æ¢ä¸ºå…¶ä»–å­—ç¬¦ä¸²çš„å­—å…¸, å¦‚: {'#': '#', ' ': ''}

        è¿”å›:
        - å¯¹è±¡åˆ—è¡¨
        """
        objects = []
        file_path = FileUtil.recookPath(file_path)
        if not FileUtil.isFileExist(file_path):
            return objects

        with open(file_path, 'r', encoding=encoding) as file:
            reader = csv.reader(file, delimiter=delimiter)

            # è·³è¿‡æŒ‡å®šè¡Œæ•°
            for _ in range(skip_rows):
                next(reader, None)

            # é€è¡Œè§£æå¹¶è½¬æ¢ä¸ºå¯¹è±¡
            for row_num, row in enumerate(reader, start=skip_rows):
                if skip_empty_line and (not row or all(not cell.strip() for cell in row)):  # è·³è¿‡ç©ºè¡Œ
                    continue

                if row[0].startswith('#') or row[0].startswith(';'):  # è·³è¿‡ä»¥ # æˆ– ; å¼€å¤´çš„è¡Œ
                    continue

                # æŒ‰éœ€å»é™¤ç­‰å·å’Œç©ºæ ¼, å°†ç­‰å·è½¬ä¸ºé€—å·,é¿å…åŸå†…å®¹ä¸­åŒ…å«å†’å·æ—¶, å†’å·ä¼šè¢«è¯†åˆ«ä¸º key-value çš„åˆ†éš”ç¬¦
                row_str = delimiter.join(row)
                ori_row_str = row_str
                if replace_dict:
                    for k, v in replace_dict.items():
                        row_str = row_str.replace(k, v)
                row = row_str.split(delimiter)

                try:
                    obj = object_class.by_csv_row(row)

                    obj.config_path = file_path
                    obj.row_number = row_num
                    obj.row_str = ori_row_str

                    objects.append(obj)
                except Exception as e:
                    print(f"è­¦å‘Š: ç¬¬{row_num}è¡Œè§£æå¤±è´¥ - {e}. è¡Œå†…å®¹: {row},oriRowStr={row_str}")
                    # å¯é€‰æ‹©è®°å½•é”™è¯¯æˆ–è·³è¿‡è¯¥è¡Œï¼Œæ­¤å¤„é€‰æ‹©è·³è¿‡

        return objects

    @staticmethod
    def extract_csv(src_path: str, column_name: str, row_ranges: List[Union[int, tuple]] = None,
                    output_path: str = None, encoding: str = 'utf-8-sig',
                    remove_empty_row: bool = True,
                    process_func: Optional[callable] = None,
                    filter_columns_dict: Optional[Dict[str, str]] = None,
                    keep_all_columns: bool = False,
                    max_rows: Optional[int] = None) -> pd.DataFrame:
        """
        ä»æŒ‡å®šCSVæ–‡ä»¶ä¸­æå–æŒ‡å®šåˆ—çš„éƒ¨åˆ†æ•°æ®ï¼Œå¹¶å¯é€‰æ‹©ä¿å­˜ä¸ºæ–°çš„CSVæ–‡ä»¶
        å‚è€ƒ extract_lines() æ–¹æ³•å®ç°
        å…¼å®¹å•ä¸ªæ•°æ®è·¨å¤šè¡Œå­˜å‚¨çš„æƒ…å†µ(å¸¦æœ‰\næ¢è¡Œç¬¦), èƒ½æ­£ç¡®è¯†åˆ«ä¸ºstræ•°æ®

        @param src_path: æºCSVæ–‡ä»¶è·¯å¾„
        @param column_name: è¦æå–çš„åˆ—åï¼Œä¾‹å¦‚ 'query'
        @param row_ranges: è¡ŒèŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„ (start_row(å«), end_row(å«)) æˆ–å•ä¸ªè¡Œå·ï¼Œè¡Œå·ä»0å¼€å§‹(ä¸åŒ…æ‹¬columnè¡Œ)
                          é»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºå¤„ç†å…¨éƒ¨æ•°æ®èŒƒå›´
        @param output_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜ä¸ºæ–°çš„CSVæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºNoneä¸ä¿å­˜
        @param encoding: æºæ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤ä¸º 'utf-8-sig'
        @param remove_empty_row: æ˜¯å¦åˆ é™¤ç©ºç™½è¡Œï¼Œé»˜è®¤ä¸ºTrue
        @param process_func: å¯é€‰çš„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¯¹æ¯ä¸ªrowæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå‡½æ•°ç­¾ååº”ä¸º func(data: str) -> str
        @param filter_columns_dict: è¿‡æ»¤æ¡ä»¶å­—å…¸ï¼Œæ ¼å¼ä¸º { åˆ—å: æ­£åˆ™è¡¨è¾¾å¼ }ï¼Œæ”¯æŒå¤šåˆ—è¿‡æ»¤
        @param keep_all_columns: æ˜¯å¦ä¿ç•™æ‰€æœ‰åˆ—æ•°æ®ï¼Œé»˜è®¤ä¸ºFalseï¼Œåªä¿ç•™æŒ‡å®šåˆ—
        @param max_rows: æœ€å¤§è¾“å‡ºè¡Œæ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨è¾“å‡ºï¼Œé»˜è®¤ä¸ºNone
        @return pd.DataFrame: æå–åçš„DataFrame
        """
        if not FileUtil.isFileExist(src_path):
            CommonUtil.printLog(f'extract_csv fail: æºæ–‡ä»¶ä¸å­˜åœ¨:{FileUtil.recookPath(src_path)}')
            return pd.DataFrame()

        try:
            # è¯»å–CSVæ–‡ä»¶ï¼Œä½¿ç”¨ dtype=str ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½ä½œä¸ºå­—ç¬¦ä¸²å¤„ç†
            # keep_default_na=False å’Œ na_values=[''] ç¡®ä¿ç©ºå€¼ä¹Ÿè¢«å½“ä½œå­—ç¬¦ä¸²å¤„ç†
            # df = pd.read_csv(src_path, encoding=encoding, dtype=str, keep_default_na=False, na_values=[''])
            df = pd.read_csv(src_path, encoding=encoding, dtype=str)

            # ç¡®ä¿æ‰€æœ‰NaNå€¼éƒ½è¢«æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            df = df.fillna('')

            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            if column_name not in df.columns:
                CommonUtil.printLog(f'extract_csv fail: åˆ— "{column_name}" ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­')
                return pd.DataFrame()

            extracted_df = df.copy()

            # å¦‚æœæ²¡æœ‰æä¾›è¿‡æ»¤å­—å…¸ï¼Œåˆ™ä¸è¿‡æ»¤æ•°æ®
            if filter_columns_dict is not None and len(filter_columns_dict) > 0:
                # æ£€æŸ¥è¿‡æ»¤åˆ—æ˜¯å¦å­˜åœ¨
                missing_columns = [col for col in filter_columns_dict.keys() if col not in extracted_df.columns]
                if missing_columns:
                    CommonUtil.printLog(f"è­¦å‘Š: åˆ— {missing_columns} ä¸å­˜åœ¨äºDataFrameä¸­")
                    return extracted_df

                # æ ¹æ®æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¿‡æ»¤
                filtered_df = extracted_df
                for filter_column, filter_keyword in filter_columns_dict.items():
                    filtered_df = filtered_df[filtered_df[filter_column].astype(str).str.contains(filter_keyword, regex=True, na=False)]
                extracted_df = filtered_df

            # æ ¹æ®keep_all_columnså‚æ•°å†³å®šæ˜¯å¦ä¿ç•™æ‰€æœ‰åˆ—
            if not keep_all_columns:
                extracted_df = df[[column_name]].copy()

            # å¤„ç†è¡ŒèŒƒå›´ç­›é€‰
            if row_ranges is None:
                # å¦‚æœrow_rangesä¸ºNoneï¼Œåˆ™å¤„ç†å…¨éƒ¨æ•°æ®èŒƒå›´
                final_df = extracted_df.reset_index(drop=True)
            else:
                # æ ¹æ®è¡ŒèŒƒå›´ç­›é€‰æ•°æ®
                selected_indices = []
                for row_range in row_ranges:
                    if isinstance(row_range, int):
                        # å•è¡Œæå–
                        if 0 <= row_range < len(extracted_df):
                            selected_indices.append(row_range)
                    elif isinstance(row_range, tuple) and len(row_range) == 2:
                        # èŒƒå›´æå–
                        start_row, end_row = row_range
                        # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        start_row = max(0, start_row)
                        end_row = min(len(extracted_df) - 1, end_row)

                        # æå–èŒƒå›´å†…çš„è¡Œ
                        if start_row <= end_row:
                            selected_indices.extend(range(start_row, end_row + 1))
                    else:
                        CommonUtil.printLog(f"extract_csv fail: æ— æ•ˆçš„è¡ŒèŒƒå›´æ ¼å¼ {row_range}")
                        continue

                # ç´¢å¼•å»é‡å¹¶æ’åºç´¢å¼•
                selected_indices = sorted(list(set(selected_indices)))

                # æ ¹æ®é€‰å®šçš„ç´¢å¼•æå–æ•°æ®
                final_df = extracted_df.iloc[selected_indices].reset_index(drop=True)

            # å¦‚æœæä¾›äº†å¤„ç†å‡½æ•°ï¼Œåˆ™å¯¹æ•°æ®è¿›è¡Œå¤„ç†
            if process_func is not None and callable(process_func):
                final_df[column_name] = final_df[column_name].apply(process_func)

            # åˆ é™¤ç©ºç™½è¡Œ
            if remove_empty_row:
                final_df = final_df[final_df[column_name].str.strip() != '']

            # é™åˆ¶è¾“å‡ºæ•°æ®é‡
            if max_rows is not None and max_rows > 0:
                final_df = final_df.head(max_rows)

            # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œåˆ™ä¿å­˜ä¸ºæ–°çš„CSVæ–‡ä»¶
            CSVUtil.to_csv(final_df, output_path, encoding=encoding)
            return final_df
        except Exception as e:
            CommonUtil.printLog(f'extract_csv_columns fail: {e}')
            return pd.DataFrame()

    @staticmethod
    def merge(df_left: pd.DataFrame, df_right: pd.DataFrame, on_column: str, priority_left: bool = True, keep_both: bool = True,
              deduplicate: bool = False):
        """
        åˆå¹¶ä¸¤ä¸ªDataFrameï¼Œå»é‡å¹¶è§£å†³å†²çª
        å¯¹äº 'on_column' åˆ—å€¼ç›¸åŒçš„è®°å½•, åªä¼šä¿ç•™ä¸€è¡Œ, è‹¥å…¶ä»–columnå€¼å­˜åœ¨å†²çª, åˆ™ä»¥ 'priority' æŒ‡å®šçš„æ•°æ®ä¸ºå‡†
        è‹¥åªæ˜¯ç®€å•çš„æ‹¼æ¥ä¸åŒçš„df,æ— éœ€å»é‡ç­‰æ“ä½œ,å¯ç›´æ¥ä½¿ç”¨åŸå§‹æ¥å£: df = pd.concat([df1, df2, df3], ignore_index=True)

        :param df_left: å·¦ä¾§DataFrame
        :param df_right: å³ä¾§DataFrame
        :param on_column: ç”¨äºå»é‡å’Œåˆå¹¶çš„å…¬å…±åˆ—å
        :param priority_left: å·¦ä¾§ DataFrameçš„å€¼åœ¨å†²çªæ—¶ä¼˜å…ˆ, è‹¥ä¸ºFalse, åˆ™å³ä¾§ DataFrameçš„å€¼ä¼˜å…ˆ
        :param keep_both: æ˜¯å¦ä¿ç•™ä¸¤ä¸ªDataFrameä¸­çš„æ‰€æœ‰è¡Œ(True: ä¿ç•™æ‰€æœ‰è¡Œ; False: åªä¿ç•™ä¼˜å…ˆçº§é«˜çš„DataFrameä¸­çš„è¡Œ)
        :param deduplicate: æ˜¯å¦åœ¨åˆå¹¶å‰å¯¹ä¸¤ä¸ªDataFrameæŒ‰on_columnå»é‡ï¼Œé»˜è®¤False
        :return: åˆå¹¶å¹¶å»é‡åçš„æœ€ç»ˆ DataFrame
        """
        # å¦‚æœéœ€è¦å»é‡ï¼Œåˆ™å…ˆå¯¹ä¸¤ä¸ªDataFrameæŒ‰on_columnå»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¡è®°å½•
        if deduplicate:
            df_left = CSVUtil.deduplicate(df_left, on_column)
            df_right = CSVUtil.deduplicate(df_right, on_column)
        else:
            # å¦‚æœä¸å»é‡ä½†ä»å¸Œæœ›é¿å…ç¬›å¡å°”ç§¯æ•ˆåº”ï¼Œéœ€è¦é¢„å¤„ç†é‡å¤è®°å½•
            # å½“keep_both=Falseæ—¶ï¼Œæˆ‘ä»¬åªä¿ç•™ä¸€ä¾§çš„æ•°æ®ï¼Œé¿å…é‡å¤è®°å½•å¯¼è‡´çš„ç¬›å¡å°”ç§¯
            if not keep_both:
                if priority_left:
                    # ä¿ç•™å·¦ä¾§DataFrameä¸­çš„æ‰€æœ‰è®°å½•ï¼Œå»é™¤å³ä¾§DataFrameä¸­ä¸å·¦ä¾§é‡å¤çš„è®°å½•ï¼ˆä½†ä¿ç•™å³ä¾§ç‹¬æœ‰çš„è®°å½•ï¼‰
                    # å…ˆæ‰¾å‡ºåœ¨å·¦ä¾§DataFrameä¸­å­˜åœ¨çš„query
                    left_queries = set(df_left[on_column])
                    # ä»å³ä¾§DataFrameä¸­ç§»é™¤ä¸å·¦ä¾§é‡å¤çš„è®°å½•ï¼Œä½†ä¿ç•™æ¯ç»„é‡å¤è®°å½•çš„ç¬¬ä¸€æ¡
                    df_right_no_duplicates = df_right[~df_right[on_column].isin(left_queries)]
                    # åˆå¹¶å³ä¾§ç‹¬æœ‰çš„è®°å½•å’Œå³ä¾§é‡å¤è®°å½•çš„ç¬¬ä¸€æ¡
                    df_right_unique = df_right[df_right[on_column].isin(left_queries)].drop_duplicates(subset=[on_column], keep='first')
                    df_right = pd.concat([df_right_no_duplicates, df_right_unique]).reset_index(drop=True)
                else:
                    # ä¿ç•™å³ä¾§DataFrameä¸­çš„æ‰€æœ‰è®°å½•ï¼Œå»é™¤å·¦ä¾§DataFrameä¸­ä¸å³ä¾§é‡å¤çš„è®°å½•ï¼ˆä½†ä¿ç•™å·¦ä¾§ç‹¬æœ‰çš„è®°å½•ï¼‰
                    # å…ˆæ‰¾å‡ºåœ¨å³ä¾§DataFrameä¸­å­˜åœ¨çš„query
                    right_queries = set(df_right[on_column])
                    # ä»å·¦ä¾§DataFrameä¸­ç§»é™¤ä¸å³ä¾§é‡å¤çš„è®°å½•ï¼Œä½†ä¿ç•™æ¯ç»„é‡å¤è®°å½•çš„ç¬¬ä¸€æ¡
                    df_left_no_duplicates = df_left[~df_left[on_column].isin(right_queries)]
                    # åˆå¹¶å·¦ä¾§ç‹¬æœ‰çš„è®°å½•å’Œå·¦ä¾§é‡å¤è®°å½•çš„ç¬¬ä¸€æ¡
                    df_left_unique = df_left[df_left[on_column].isin(right_queries)].drop_duplicates(subset=[on_column], keep='first')
                    df_left = pd.concat([df_left_no_duplicates, df_left_unique]).reset_index(drop=True)

        if keep_both:
            # ä¿ç•™ä¸¤ä¸ªDataFrameçš„æ‰€æœ‰è¡Œï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            if not priority_left:
                df_left, df_right = df_right, df_left  # äº¤æ¢ä½ç½®ï¼Œç»Ÿä¸€æŒ‰å·¦ä¼˜å…ˆå¤„ç†

            # 1. åˆå¹¶
            merged_df = pd.merge(df_left, df_right, on=on_column, how='outer', suffixes=('_left', '_right'))

            # 2. è§£å†³å†²çª
            right_columns = [col for col in merged_df.columns if col.endswith('_right')]
            for right_col in right_columns:
                left_col = right_col.replace('_right', '_left')
                # å½“å·¦ä¾§å€¼ä¸ºç©ºæ—¶ï¼Œä½¿ç”¨å³ä¾§å€¼ï¼›å¦åˆ™ä½¿ç”¨å·¦ä¾§å€¼
                merged_df[right_col] = np.where(
                    (merged_df[left_col].isna()) | (merged_df[left_col] == ''),
                    merged_df[right_col],
                    merged_df[left_col]
                )
                merged_df.drop(columns=[left_col], inplace=True)
                merged_df.rename(columns={right_col: right_col.replace('_right', '')}, inplace=True)

            # å¯¹äºå·¦ä¾§DataFrameä¸­å®Œå…¨ä¸ºç©ºçš„åˆ—ï¼Œä½¿ç”¨å³ä¾§DataFrameä¸­çš„å€¼è¿›è¡Œå¡«å……
            for col in merged_df.columns:
                if col != on_column and not col.endswith('_left') and not col.endswith('_right'):
                    # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨å³ä¾§DataFrameä¸­å­˜åœ¨
                    right_col_name = col + '_right'
                    if right_col_name in merged_df.columns:
                        # å¦‚æœå½“å‰åˆ—ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å³ä¾§åˆ—çš„å€¼
                        merged_df[col] = np.where(
                            (merged_df[col].isna()) | (merged_df[col] == ''),
                            merged_df[right_col_name],
                            merged_df[col]
                        )
                        # åˆ é™¤å³ä¾§åˆ—
                        merged_df.drop(columns=[right_col_name], inplace=True)

            return merged_df
        else:
            # åªä¿ç•™ä¼˜å…ˆçº§é«˜çš„DataFrameä¸­çš„è¡Œ
            if priority_left:
                # ä¿ç•™å·¦ä¾§DataFrameä¸­çš„è¡Œï¼Œå¦‚æœæœ‰å†²çªåˆ™ä½¿ç”¨å·¦ä¾§çš„å€¼
                merged_df = pd.merge(df_left, df_right, on=on_column, how='left', suffixes=('_left', '_right'))

                # è§£å†³å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å·¦ä¾§çš„å€¼
                right_columns = [col for col in merged_df.columns if col.endswith('_right')]
                for right_col in right_columns:
                    left_col = right_col.replace('_right', '_left')
                    # ä¼˜å…ˆä½¿ç”¨å·¦ä¾§çš„å€¼ï¼Œå¦‚æœå·¦ä¾§ä¸ºç©ºåˆ™ä½¿ç”¨å³ä¾§çš„å€¼
                    merged_df[left_col] = np.where(
                        (merged_df[left_col].isna()) | (merged_df[left_col] == ''),
                        merged_df[right_col],
                        merged_df[left_col]
                    )
                    merged_df.drop(columns=[right_col], inplace=True)
                    merged_df.rename(columns={left_col: left_col.replace('_left', '')}, inplace=True)

                # å¯¹äºå·¦ä¾§DataFrameä¸­å®Œå…¨ä¸ºç©ºçš„åˆ—ï¼Œä½¿ç”¨å³ä¾§DataFrameä¸­çš„å€¼è¿›è¡Œå¡«å……
                for col in merged_df.columns:
                    if col != on_column and not col.endswith('_left') and not col.endswith('_right'):
                        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨å³ä¾§DataFrameä¸­å­˜åœ¨
                        right_col_name = col + '_right'
                        if right_col_name in merged_df.columns:
                            # å¦‚æœå½“å‰åˆ—ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å³ä¾§åˆ—çš„å€¼
                            merged_df[col] = np.where(
                                (merged_df[col].isna()) | (merged_df[col] == ''),
                                merged_df[right_col_name],
                                merged_df[col]
                            )
                            # åˆ é™¤å³ä¾§åˆ—
                            merged_df.drop(columns=[right_col_name], inplace=True)

                return merged_df
            else:
                # ä¿ç•™å³ä¾§DataFrameä¸­çš„è¡Œï¼Œå¦‚æœæœ‰å†²çªåˆ™ä½¿ç”¨å³ä¾§çš„å€¼
                merged_df = pd.merge(df_left, df_right, on=on_column, how='right', suffixes=('_left', '_right'))

                # è§£å†³å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å³ä¾§çš„å€¼
                right_columns = [col for col in merged_df.columns if col.endswith('_right')]
                for right_col in right_columns:
                    left_col = right_col.replace('_right', '_left')
                    # ä¼˜å…ˆä½¿ç”¨å³ä¾§çš„å€¼ï¼Œå¦‚æœå³ä¾§ä¸ºç©ºåˆ™ä½¿ç”¨å·¦ä¾§çš„å€¼
                    merged_df[right_col] = np.where(
                        (merged_df[right_col].isna()) | (merged_df[right_col] == ''),
                        merged_df[left_col],
                        merged_df[right_col]
                    )
                    merged_df.drop(columns=[left_col], inplace=True)
                    merged_df.rename(columns={right_col: right_col.replace('_right', '')}, inplace=True)

                # å¯¹äºå³ä¾§DataFrameä¸­å®Œå…¨ä¸ºç©ºçš„åˆ—ï¼Œä½¿ç”¨å·¦ä¾§DataFrameä¸­çš„å€¼è¿›è¡Œå¡«å……
                for col in merged_df.columns:
                    if col != on_column and not col.endswith('_left') and not col.endswith('_right'):
                        # æ£€æŸ¥è¯¥åˆ—æ˜¯å¦åœ¨å·¦ä¾§DataFrameä¸­å­˜åœ¨
                        left_col_name = col + '_left'
                        if left_col_name in merged_df.columns:
                            # å¦‚æœå½“å‰åˆ—ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å·¦ä¾§åˆ—çš„å€¼
                            merged_df[col] = np.where(
                                (merged_df[col].isna()) | (merged_df[col] == ''),
                                merged_df[left_col_name],
                                merged_df[col]
                            )
                            # åˆ é™¤å·¦ä¾§åˆ—
                            merged_df.drop(columns=[left_col_name], inplace=True)

                return merged_df

    @staticmethod
    def merge_csv(csv1: str, csv2: str, on_column: str,
                  priority_left: bool = True,
                  encoding: str = 'utf-8-sig',
                  output_csv: Optional[str] = None,
                  keep_both: bool = True,
                  deduplicate: bool = False
                  ) -> pd.DataFrame:
        """
        åˆå¹¶ä¸¤ä¸ªCSVæ–‡ä»¶ï¼Œå¹¶å¯ä»¥æŒ‡å®šä»¥å“ªä¸ªæ–‡ä»¶ä¸ºå‡†

        :param csv1: ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶è·¯å¾„ (ä½œä¸º 'left')
        :param csv2: ç¬¬äºŒä¸ªCSVæ–‡ä»¶è·¯å¾„ (ä½œä¸º 'right')
        :param on_column: ç”¨äºåˆå¹¶çš„å…¬å…±åˆ—å
        :param priority_left: å·¦ä¾§ DataFrameçš„å€¼åœ¨å†²çªæ—¶ä¼˜å…ˆ, è‹¥ä¸ºFalse, åˆ™å³ä¾§ DataFrameçš„å€¼ä¼˜å…ˆ
        :param output_csv: å°†åˆå¹¶åçš„ç»“æœå†™å…¥çš„è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ (å¯é€‰), Noneè¡¨ç¤ºä¸è¾“å‡º
        :param encoding: CSVæ–‡ä»¶çš„ç¼–ç 
        :param keep_both: æ˜¯å¦ä¿ç•™ä¸¤ä¸ªDataFrameä¸­çš„æ‰€æœ‰è¡Œ(True: ä¿ç•™æ‰€æœ‰è¡Œ; False: åªä¿ç•™ä¼˜å…ˆçº§é«˜çš„DataFrameä¸­çš„è¡Œ)
        :param deduplicate: æ˜¯å¦åœ¨åˆå¹¶å‰å¯¹ä¸¤ä¸ªDataFrameæŒ‰on_columnå»é‡ï¼Œé»˜è®¤False
        :return: åˆå¹¶åçš„ DataFrame
        """
        df1 = CSVUtil.read_csv(csv1, encoding=encoding)
        df2 = CSVUtil.read_csv(csv2, encoding=encoding)

        merged_df = CSVUtil.merge(df1, df2, on_column, priority_left, keep_both, deduplicate)
        CSVUtil.to_csv(merged_df, output_csv, encoding=encoding, index=False)
        return merged_df

    @staticmethod
    def calc_accuracy(df: pd.DataFrame, column_base: str, column_compare: str, keyword: Optional[str] = None,
                      keyword_col: Optional[str] = None, enable_any_empty: bool = False, enable_all_empty: bool = False) -> dict:
        """
        è®¡ç®—æŒ‡å®šåˆ—çš„å‡†ç¡®ç‡ç»Ÿè®¡ä¿¡æ¯:
        1. è¿‡æ»¤ column_base å’Œ column_compare å‡æœ‰å€¼çš„æ•°æ®
            ä¼šç”Ÿæˆä¸€ä¸ªdataFrame, è®°ä¸º: valid_df
            å¯¹åº”çš„æ•°æ®é‡, è®°ä¸º: valid_cnt  å³: len(valid_df)
        2. è®¡ç®— valid_df ä¸­ column_base å’Œ column_compare ç›¸åŒå€¼çš„æ•°é‡
            ä¼šç”Ÿæˆä¸€ä¸ªdataFrame, è®°ä¸º: same_df
            å¯¹åº”çš„æ•°æ®é‡, è®°ä¸º: same_cnt  å³: len(same_df)
        3. è®¡ç®—å‡†ç¡®ç‡, è®°ä¸º: accuracy = same_cnt / valid_cnt

        Args:
            df (pandas.DataFrame): æ•°æ®æ¡†
            column_base (str): åŸºå‡†æ•°æ®åˆ—å, æ­¤ä¸ºçœŸå€¼åˆ—
            column_compare (str): å¾…ç»Ÿè®¡å‡†ç¡®ç‡çš„åˆ—å, æ­¤ä¸ºé¢„æµ‹å€¼åˆ—
            keyword (str, optional): å…³é”®å­—è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚æœæä¾›ä¸”keyword_colä¸ä¸ºç©ºï¼Œåˆ™ keyword_col åˆ—çš„å€¼å¿…é¡»åŒ…å«è¯¥å…³é”®å­—æ‰è¢«è§†ä¸ºæœ‰æ•ˆæ•°æ®
            keyword_col (str, optional): å…³é”®å­—æ‰€åœ¨çš„åˆ—åï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æ»¡è¶³æ¡ä»¶ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™é»˜è®¤æ˜¯: column_compare
            enable_any_empty (bool, optional): æ˜¯å¦å…è®¸ä»»æ„ä¸€åˆ—æ˜¯ç©ºã€‚è‹¥ä¸ºTrueï¼Œåˆ™å…è®¸åˆ—å€¼ä¸ºç©ºå­—ç¬¦ä¸²''æˆ–NaN è‹¥ä¸ºFalse,åˆ™ä¸¤åˆ—éƒ½è¦æ±‚éç©º
            enable_all_empty (bool, optional): æ˜¯å¦å…è®¸æ‰€æœ‰åˆ—æ˜¯ç©ºã€‚è‹¥ä¸ºTrueï¼Œåˆ™å…è®¸æ‰€æœ‰åˆ—éƒ½ä¸ºç©º, ä¸åšè¿‡æ»¤, ä¼˜å…ˆçº§é«˜äºenable_any_empty

        Returns:
            dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼Œå„keyå«ä¹‰å¦‚ä¸‹ï¼š
                - total_cnt (int): æ€»æ•°æ®æ•°ï¼Œå³æ•°æ®æ¡†çš„æ€»è¡Œæ•°
                - valid_cnt (int): æœ‰æ•ˆæ•°æ®æ•°ï¼Œå³ä¸¤ä¸ªåˆ—å‡æœ‰å€¼çš„è¡Œæ•°, è‹¥ enable_empty=True, åˆ™å…è®¸å€¼ä¸ºç©º
                - same_cnt (int): åŒ¹é…æ•°æ®æ•°ï¼Œå³ä¸¤ä¸ªåˆ—å€¼ç›¸ç­‰çš„è¡Œæ•°
                - accuracy (float): å‡†ç¡®ç‡ï¼Œè®¡ç®—å…¬å¼ä¸º same_cnt/valid_cnt
                - valid_df (pandas.DataFrame): æœ‰æ•ˆæ•°æ®çš„DataFrameï¼Œå³ä¸¤ä¸ªåˆ—å‡æœ‰å€¼çš„æ•°æ®å­é›†
                - same_df (pandas.DataFrame): åŒ¹é…æ•°æ®çš„DataFrameï¼Œå³ä¸¤ä¸ªåˆ—å€¼ç›¸ç­‰çš„æ•°æ®å­é›†
        """
        # 1. æ€»æ•°æ®æ•°
        total_cnt = len(df)

        # 2. æ ¹æ® enable_empty å‚æ•°è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        if enable_all_empty:  # å…è®¸ä¸¤åˆ—éƒ½ä¸ºç©º,åˆ™æ— éœ€åšè¿‡æ»¤
            valid_df = df
        elif enable_any_empty:  # å…è®¸ä»»æ„ä¸€åˆ—ä¸ºç©º
            # æ³¨æ„ï¼šNaNè¡¨ç¤ºæ•°æ®ç¼ºå¤±ï¼Œç©ºå­—ç¬¦ä¸²''è¡¨ç¤ºæ•°æ®å­˜åœ¨ä½†ä¸ºç©º
            valid_df = df[(df[column_base].notna()) | (df[column_compare].notna())]
        else:
            # ä¸å…è®¸ç©ºå€¼ï¼šä¸¤åˆ—éƒ½æœ‰å€¼ä¸”ä¸ä¸ºç©ºå­—ç¬¦ä¸²
            valid_df = df[
                (df[column_base].notna()) &
                (df[column_compare].notna()) &
                (df[column_base].astype(str).str.strip() != '') &
                (df[column_compare].astype(str).str.strip() != '')
                ]
            # valid_df = df[df[column_base].notna() & df[column_compare].notna()]

        # 3. å¦‚æœæä¾›äº†keywordå’Œkeyword_colå‚æ•°ï¼Œåˆ™è¿›ä¸€æ­¥è¿‡æ»¤keyword_colåŒ…å«å…³é”®å­—çš„æ•°æ®
        keyword_col = column_compare if keyword_col is None else keyword_col
        if keyword is not None and keyword_col is not None and keyword_col in df.columns:
            valid_df = valid_df[valid_df[keyword_col].astype(str).str.contains(keyword, na=False)]

        valid_cnt = len(valid_df)

        # 4. column_base å’Œ column_compare å€¼ç›¸ç­‰çš„æ•°æ®é‡åŠå¯¹åº”DataFrame
        same_df = valid_df[valid_df[column_base].astype(str) == valid_df[column_compare].astype(str)]
        same_cnt = len(same_df)

        # 5. å‡†ç¡®ç‡è®¡ç®—
        accuracy = same_cnt / valid_cnt if valid_cnt > 0 else 0

        return {
            'total_cnt': total_cnt,  # æ€»æ•°æ®æ•°
            'valid_cnt': valid_cnt,  # æœ‰æ•ˆæ•°æ®æ•°ï¼ˆä¸¤åˆ—å‡æœ‰å€¼ï¼Œä¸”æ»¡è¶³keywordæ¡ä»¶ï¼‰
            'same_cnt': same_cnt,  # åŒ¹é…æ•°æ®æ•°ï¼ˆä¸¤åˆ—å€¼ç›¸ç­‰ï¼‰
            'accuracy': accuracy,  # å‡†ç¡®ç‡ï¼ˆåŒ¹é…æ•°/æœ‰æ•ˆæ•°ï¼‰
            'valid_df': valid_df,  # æœ‰æ•ˆæ•°æ®DataFrame
            'same_df': same_df  # åŒ¹é…æ•°æ®DataFrame
        }

    @staticmethod
    def calc_csv_accuracy(csv_path: str, column_base: str, column_compare: str,
                          keyword: Optional[str] = None, keyword_col: Optional[str] = None, encoding: str = 'utf-8-sig',
                          enable_any_empty: bool = False, enable_all_empty: bool = False):
        """
        è®¡ç®—CSVæ–‡ä»¶æŒ‡å®šåˆ—çš„å‡†ç¡®ç‡

        Args:
            csv_path (str): CSVæ–‡ä»¶è·¯å¾„
            column_base (str): åŸºå‡†æ•°æ®åˆ—å, æ­¤ä¸ºçœŸå€¼åˆ—
            column_compare (str): å¾…ç»Ÿè®¡å‡†ç¡®ç‡çš„åˆ—å, æ­¤ä¸ºé¢„æµ‹å€¼åˆ—
            keyword (str, optional): å…³é”®å­—è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚æœæä¾›ä¸”keyword_colä¸ä¸ºç©ºï¼Œåˆ™ keyword_col åˆ—çš„å€¼å¿…é¡»åŒ…å«è¯¥å…³é”®å­—æ‰è¢«è§†ä¸ºæœ‰æ•ˆæ•°æ®
            keyword_col (str, optional): å…³é”®å­—æ‰€åœ¨çš„åˆ—åï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æ»¡è¶³æ¡ä»¶ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™é»˜è®¤æ˜¯: column_compare
            encoding (str): CSVæ–‡ä»¶çš„ç¼–ç ï¼Œé»˜è®¤ä¸º 'utf-8-sig'
            enable_any_empty (bool, optional): æ˜¯å¦å…è®¸ä»»æ„ä¸€åˆ—æ˜¯ç©ºã€‚è‹¥ä¸ºTrueï¼Œåˆ™å…è®¸åˆ—å€¼ä¸ºç©º
            enable_all_empty (bool, optional): æ˜¯å¦å…è®¸æ‰€æœ‰åˆ—æ˜¯ç©ºã€‚è‹¥ä¸ºTrueï¼Œåˆ™å…è®¸æ‰€æœ‰åˆ—éƒ½ä¸ºç©º,ä¼˜å…ˆçº§é«˜äºenable_any_empty

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«å‡†ç¡®ç‡ã€æœ‰æ•ˆæ•°æ®æ•°ã€åŒ¹é…æ•°æ®æ•°ã€æ€»æ•°æ®æ•°ç­‰ä¿¡æ¯
        """
        df = pd.read_csv(csv_path, encoding=encoding, dtype=str)
        return CSVUtil.calc_accuracy(df, column_base, column_compare, keyword, keyword_col, enable_any_empty, enable_all_empty)

    @staticmethod
    def to_markdown(df: pd.DataFrame, include_index: bool = True, output_file: Optional[str] = None, encoding: str = 'utf-8-sig',
                    title: Optional[str] = None, append: bool = False, align_flag: str = ':---:') -> str:
        """
        å°† DataFrame è½¬æ¢ä¸º Markdown è¡¨æ ¼å­—ç¬¦ä¸²,å¹¶æŒ‰éœ€å­˜å‚¨åˆ°æ–‡ä»¶ä¸­
        :param df: è¾“å…¥çš„ DataFrame
        :param include_index: æ˜¯å¦åŒ…å«ç´¢å¼•åˆ—ï¼ˆé»˜è®¤ä¸º Trueï¼‰
        :param output_file: è¾“å‡ºçš„ Markdown æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        :param encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ä¸º 'utf-8-sig'ï¼‰
        :param title: è¡¨æ ¼æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™åœ¨è¡¨æ ¼ç¬¬ä¸€è¡Œæ·»åŠ æ ‡é¢˜è¡Œ
        :param append: æ˜¯å¦è¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾ï¼ˆé»˜è®¤ä¸º Falseï¼‰
        :param align_flag: å¯¹é½æ–¹å¼ å±…ä¸­: ':---:'   å·¦å¯¹é½: ':---' å³å¯¹é½: '---:'
        """
        if include_index:
            n_df = df.reset_index()  # å°†indexå˜æˆæ•°æ®åˆ—,å¹¶è¿”å›ä¸€ä¸ªè¡Œçš„df
        else:
            n_df = df

        markdown_str = "| " + " | ".join(n_df.columns) + " |\n"  # è¡¨å¤´-åˆ—å

        # æ·»åŠ åˆ†éš”è¡Œï¼ˆå¿…é¡»åœ¨è¡¨å¤´ä¹‹åã€æ•°æ®è¡Œä¹‹å‰ï¼‰
        markdown_str += "| " + " | ".join([align_flag] * len(n_df.columns)) + " |\n"

        # # æ·»åŠ è¡¨æ ¼æ ‡é¢˜è¡Œï¼ˆå¦‚æœæä¾›ï¼‰
        # if title:
        #     # åˆ›å»ºæ ‡é¢˜è¡Œï¼Œå°†æ ‡é¢˜æ”¾åœ¨ç¬¬ä¸€åˆ—ï¼Œå…¶ä½™åˆ—ä¸ºç©º
        #     title_row = f"| {title} "
        #     for i in range(len(n_df.columns) - 1):
        #         title_row += "| "
        #     title_row += "|\n"
        #     title_row += "| " + " | ".join([":---:"] * len(n_df.columns)) + " |\n"
        #     markdown_str = title_row + markdown_str

        for _, row in n_df.iterrows():
            markdown_str += "| " + " | ".join(str(v) for v in row) + " |\n"

        if not CommonUtil.isNoneOrBlank(title):
            markdown_str = f'**{title}**\n\n' + markdown_str

        if output_file:
            if append:
                FileUtil.append2File(output_file, markdown_str, encoding=encoding)
            else:
                FileUtil.write2File(output_file, markdown_str, encoding=encoding)

        return markdown_str

    @staticmethod
    def filter_and_replace(
            df: pd.DataFrame,
            filter_columns_dict: Optional[Dict[str, str]] = None,
            row_ranges: Optional[List[Union[int, tuple]]] = None,
            replace_columns_dict: Optional[Dict[str, str]] = None,
    ) -> pd.DataFrame:
        """
        å¯¹ä¼ å…¥çš„DataFrameè¿›è¡Œæ‹·è´ç„¶åè¿‡æ»¤ï¼Œå¹¶åœ¨æŒ‡å®šèŒƒå›´å†…æ›¿æ¢æŒ‡å®šåˆ—çš„æ•°æ®

        @param df: è¾“å…¥çš„DataFrame
        @param filter_columns_dict: è¿‡æ»¤æ¡ä»¶å­—å…¸ï¼Œæ ¼å¼ä¸º { åˆ—å: æ­£åˆ™è¡¨è¾¾å¼ }ï¼Œæ”¯æŒå¤šåˆ—è¿‡æ»¤
        @param row_ranges: æ›¿æ¢çš„è¡ŒèŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„ (start_row, end_row) æˆ–å•ä¸ªè¡Œå·ï¼Œ
                          è¡Œå·ç›¸å¯¹äºè¿‡æ»¤åçš„DataFrameï¼Œä»0å¼€å§‹ã€‚
                          é»˜è®¤ä¸ºNoneï¼Œè¡¨ç¤ºæ›¿æ¢æ‰€æœ‰è¡Œ
        @param replace_columns_dict: éœ€è¦æ›¿æ¢æ•°æ®çš„åˆ—åå­—å…¸ï¼Œæ ¼å¼ä¸º {åˆ—å: æ›¿æ¢å€¼}
        @return: å¤„ç†åçš„DataFrame
                å¦‚æœåªè¿‡æ»¤ä¸æ›¿æ¢, åˆ™è¿”å›è¿‡æ»¤åçš„DataFrame(ä»…åŒ…å«ç¬¦åˆæ¡ä»¶çš„è¡Œ)
                è‹¥æœ‰æ›¿æ¢, åˆ™è¿”å›æ›¿æ¢åçš„å®Œæ•´DataFrame(åŒ…å«æ‰€æœ‰è¡Œï¼Œä½†ç›¸å…³æ•°æ®å·²è¢«æ›¿æ¢)
        """
        # å¦‚æœæ²¡æœ‰æä¾›è¿‡æ»¤å­—å…¸ï¼Œåˆ™ä¸è¿‡æ»¤æ•°æ®
        if filter_columns_dict is not None and len(filter_columns_dict) > 0:
            # æ£€æŸ¥è¿‡æ»¤åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = [col for col in filter_columns_dict.keys() if col not in df.columns]
            if missing_columns:
                CommonUtil.printLog(f"è­¦å‘Š: åˆ— {missing_columns} ä¸å­˜åœ¨äºDataFrameä¸­")
                return df

            # æ ¹æ®æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¿‡æ»¤
            filtered_df = df
            for filter_column, filter_keyword in filter_columns_dict.items():
                filtered_df = filtered_df[filtered_df[filter_column].astype(str).str.contains(filter_keyword, regex=True, na=False)]
        else:
            # ä¸è¿‡æ»¤ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
            filtered_df = df

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¦æ›¿æ¢çš„åˆ—ï¼Œåˆ™ç›´æ¥è¿”å›è¿‡æ»¤åçš„DataFrame
        if not replace_columns_dict:
            return filtered_df

        # ç¡®ä¿è¦æ›¿æ¢çš„åˆ—å­˜åœ¨
        filtered_df = CSVUtil.add_cols(filtered_df, list(replace_columns_dict.keys()))

        # ç¡®å®šè¦æ›¿æ¢çš„è¡Œç´¢å¼•
        if row_ranges is None:
            # é»˜è®¤æ›¿æ¢æ‰€æœ‰è¿‡æ»¤å‡ºçš„è¡Œ
            replace_indices = filtered_df.index.tolist()
        else:
            # æ ¹æ®æŒ‡å®šèŒƒå›´ç¡®å®šè¦æ›¿æ¢çš„è¡Œ
            replace_indices = []
            for row_range in row_ranges:
                if isinstance(row_range, int):
                    # å•è¡Œæ›¿æ¢
                    if 0 <= row_range < len(filtered_df):
                        replace_indices.append(filtered_df.index[row_range])
                elif isinstance(row_range, tuple) and len(row_range) == 2:
                    # èŒƒå›´æ›¿æ¢
                    start_row, end_row = row_range
                    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    start_row = max(0, start_row)
                    end_row = min(len(filtered_df) - 1, end_row)

                    # æ·»åŠ èŒƒå›´å†…çš„è¡Œç´¢å¼•
                    if start_row <= end_row:
                        replace_indices.extend(filtered_df.index[start_row:end_row + 1])
                else:
                    CommonUtil.printLog(f"è­¦å‘Š: æ— æ•ˆçš„è¡ŒèŒƒå›´æ ¼å¼ {row_range}")

        # æ‰§è¡Œæ›¿æ¢æ“ä½œ(åœ¨å®Œæ•´DataFrameä¸Šè¿›è¡Œæ›¿æ¢)
        for col, value in replace_columns_dict.items():
            df.loc[replace_indices, col] = value

        # å¦‚æœæœ‰æ›¿æ¢æ“ä½œï¼Œè¿”å›å®Œæ•´çš„DataFrame
        return df

    @staticmethod
    def sample_by_column_values(df: pd.DataFrame, column_name: str, value_counts_dict: Dict[str, int], balance_counts: bool = False) -> pd.DataFrame:
        """
        æŒ‰æŒ‡å®šåˆ—çš„ä¸åŒå€¼éšæœºæŠ½æ ·

        Args:
            df: æºDataFrame
            column_name: è¦ç­›é€‰çš„åˆ—å
            value_counts_dict: å­—å…¸ï¼Œkeyä¸ºè¦ç­›é€‰çš„å€¼ï¼Œvalueä¸ºæ¯ä¸ªå€¼è¦æŠ½å–çš„è¡Œæ•°
            balance_counts: æ˜¯å¦å¹³è¡¡æ¯ä¸ªå€¼çš„æŠ½å–æ•°é‡ï¼Œé»˜è®¤ä¸ºFalse, è‹¥ä¸ºTrue,åˆ™ä¼šä»¥å®é™…å„ç±»å¯å–æ•°é‡çš„æœ€å°å€¼ä½œä¸ºæœ€ç»ˆè·å–æ•°

        Returns:
            æŠ½æ ·åçš„DataFrame
        """

        if balance_counts:
            # å¹³è¡¡æ¨¡å¼ï¼šå„ç±»åˆ«æ•°æ®é‡ä¿æŒä¸€è‡´
            min_cnt = len(df)
            for value, cnt in value_counts_dict.items():
                filtered = df[df[column_name] == value]
                final_cnt = min(len(filtered), cnt)
                min_cnt = min(min_cnt, final_cnt)

            for key in value_counts_dict:
                value_counts_dict[key] = min_cnt

        sampled_dfs = []

        for value, count in value_counts_dict.items():
            # ç­›é€‰å‡ºè¯¥å€¼çš„æ‰€æœ‰è¡Œ
            filtered_df = df[df[column_name] == value]

            # å¦‚æœè¯¥å€¼çš„è¡Œæ•°å°‘äºè¦æ±‚çš„æ•°é‡ï¼Œåˆ™å–å…¨éƒ¨
            # å¦åˆ™éšæœºæŠ½å–æŒ‡å®šæ•°é‡
            if len(filtered_df) <= count:
                sampled_dfs.append(filtered_df)
            else:
                sampled_dfs.append(filtered_df.sample(n=count, random_state=42))

        # åˆå¹¶æ‰€æœ‰æŠ½æ ·çš„ç»“æœ
        if sampled_dfs:
            result_df = pd.concat(sampled_dfs, ignore_index=True)
            # æ‰“ä¹±æœ€ç»ˆç»“æœçš„é¡ºåº
            result_df = result_df.sample(frac=1, random_state=42).reset_index(drop=True)
            return result_df
        else:
            return pd.DataFrame()

    @staticmethod
    def deduplicate(df: pd.DataFrame, subset: Union[str, List[str]], keep: str = 'first') -> pd.DataFrame:
        """
        åŸºäºæŒ‡å®šåˆ—å¯¹DataFrameè¿›è¡Œå»é‡æ“ä½œ, è¿”å›å»é‡åçš„DataFrame (ä¸ä¿®æ”¹åŸDataFrame)
        P.S. å¦‚æœéœ€è¦ä¿®æ”¹åŸDataFrame, è¯·ä½¿ç”¨: df.drop_duplicates(subset=subset, keep=keep, inplace=True)

        :param df: å¾…å»é‡çš„DataFrame
        :param subset: ç”¨äºåˆ¤æ–­é‡å¤çš„åˆ—åæˆ–åˆ—ååˆ—è¡¨ï¼Œå¦‚: 'column_name' æˆ– ['col1', 'col2']
        :param keep: ä¿ç•™ç­–ç•¥ï¼Œ'first'(é»˜è®¤)ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•ï¼Œ'last' ä¿ç•™æœ€åä¸€æ¬¡å‡ºç°çš„è®°å½•ï¼ŒFalse åˆ é™¤æ‰€æœ‰é‡å¤é¡¹
        :return: å»é‡åçš„DataFrame
        """
        return df.drop_duplicates(subset=subset, keep=keep).reset_index(drop=True)

    @staticmethod
    def find_duplicate_rows(df: pd.DataFrame, subset: Union[str, List[str]]) -> pd.DataFrame:
        """
        æŸ¥æ‰¾å¹¶è¿”å›åŸºäºæŒ‡å®šåˆ—çš„é‡å¤è¡Œï¼Œå°†é‡å¤çš„è¡Œæ’åˆ—åœ¨ä¸€èµ·(ä¿æŒåŸå§‹ç´¢å¼•)ï¼Œä¾¿äºå¿«é€ŸæŸ¥çœ‹é‡å¤å†…å®¹
        P.S. è¿”å›çš„dataframeä¼šä¿ç•™åŸç´¢å¼•,è‹¥éœ€è¦é‡ç½®ç´¢å¼•, è¯·ä½¿ç”¨: result_df.reset_index(drop=True)

        :param df: è¾“å…¥çš„DataFrame
        :param subset: ç”¨äºåˆ¤æ–­é‡å¤çš„åˆ—åæˆ–åˆ—ååˆ—è¡¨ï¼Œå¦‚: 'column_name' æˆ– ['col1', 'col2']
        :return: åŒ…å«æ‰€æœ‰é‡å¤è¡Œçš„DataFrameï¼ŒæŒ‰é‡å¤ç»„æ’åˆ—
        """
        # æ‰¾åˆ°æ‰€æœ‰é‡å¤çš„è¡Œï¼ˆåŒ…æ‹¬é¦–æ¬¡å‡ºç°çš„è¡Œï¼‰
        duplicate_mask = df.duplicated(subset=subset, keep=False)
        duplicates_df = df[duplicate_mask]

        # å¦‚æœåªæœ‰ä¸€åˆ—ç”¨äºåˆ¤æ–­é‡å¤ï¼Œç›´æ¥æŒ‰è¯¥åˆ—æ’åº
        if isinstance(subset, str):
            sorted_df = duplicates_df.sort_values(by=[subset])
        else:
            # å¦‚æœæ˜¯å¤šåˆ—ï¼ŒæŒ‰è¿™äº›åˆ—æ’åºï¼Œä½¿é‡å¤çš„è¡Œèšé›†åœ¨ä¸€èµ·
            sorted_df = duplicates_df.sort_values(by=subset)

        # return sorted_df.reset_index(drop=True) # é‡ç½®ç´¢å¼•
        return sorted_df

    @staticmethod
    def filter_matching_columns(df: pd.DataFrame, column_pairs: Dict[str, str], all_match: bool = True, same_value: bool = True):
        """
        æ‰¾å‡ºDataFrameä¸­æŒ‡å®šåˆ—å¯¹å€¼ç›¸åŒæˆ–ä¸åŒçš„æ•°æ®è¡Œ

        å‚æ•°:
        df: pandas DataFrame
        column_pairs: dict, é”®å€¼å¯¹è¡¨ç¤ºéœ€è¦æ¯”è¾ƒçš„åˆ—, å¦‚ {'A': 'B', 'C': 'D'}
        all_match: bool, Trueè¡¨ç¤ºæ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³(AND)ï¼ŒFalseè¡¨ç¤ºä»»æ„æ¡ä»¶æ»¡è¶³(OR)
        same_value: bool, Trueè¡¨ç¤ºæŸ¥æ‰¾å€¼ç›¸åŒçš„è¡Œï¼ŒFalseè¡¨ç¤ºæŸ¥æ‰¾å€¼ä¸åŒçš„è¡Œ

        è¿”å›:
        è¿‡æ»¤åçš„DataFrameï¼ŒåªåŒ…å«æ»¡è¶³æ¡ä»¶çš„è¡Œ
        """
        if not column_pairs:
            return df

        # å®šä¹‰é€»è¾‘æ“ä½œå‡½æ•°æ›¿ä»£lambdaè¡¨è¾¾å¼
        def and_operation(x, y):
            return x & y

        def or_operation(x, y):
            return x | y

        def same_compare(col1_param, col2_param):
            return df[col1_param] == df[col2_param]

        def diff_compare(col1_param, col2_param):
            return df[col1_param] != df[col2_param]

        # æ ¹æ®all_matché€‰æ‹©é€»è¾‘æ“ä½œ
        if all_match:
            mask = True
            op = and_operation
        else:
            mask = False
            op = or_operation

        # æ ¹æ®same_valueé€‰æ‹©æ¯”è¾ƒæ“ä½œç¬¦
        if same_value:
            compare_op = same_compare
        else:
            compare_op = diff_compare

        # éå†æ‰€æœ‰åˆ—å¯¹ï¼Œæ£€æŸ¥æ¯ä¸€å¯¹åˆ—çš„å€¼æ˜¯å¦ç¬¦åˆæ¡ä»¶
        for col1, col2 in column_pairs.items():
            condition = compare_op(col1, col2)
            mask = op(mask, condition)

        # è¿”å›ç¬¦åˆæ¡ä»¶çš„è¡Œ
        return df[mask]

    @staticmethod
    def filter_containing_columns(df: pd.DataFrame, column_pairs: Dict[str, str], all_match: bool = True):
        """
        æ‰¾å‡ºDataFrameä¸­æŒ‡å®šåˆ—å¯¹å€¼å­˜åœ¨åŒ…å«å…³ç³»çš„æ•°æ®è¡Œ

        å‚æ•°:
        df: pandas DataFrame
        column_pairs: dict, é”®å€¼å¯¹è¡¨ç¤ºéœ€è¦æ¯”è¾ƒåŒ…å«å…³ç³»çš„åˆ—, å¦‚ {'A': 'B', 'C': 'D'} è¡¨ç¤ºAåŒ…å«B, CåŒ…å«D
        all_match: bool, Trueè¡¨ç¤ºæ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³(AND)ï¼ŒFalseè¡¨ç¤ºä»»æ„æ¡ä»¶æ»¡è¶³(OR)

        è¿”å›:
        è¿‡æ»¤åçš„DataFrameï¼ŒåªåŒ…å«æ»¡è¶³åŒ…å«å…³ç³»æ¡ä»¶çš„è¡Œ
        """
        if not column_pairs:
            return df

        # å®šä¹‰é€»è¾‘æ“ä½œå‡½æ•°
        def and_operation(x, y):
            return x & y

        def or_operation(x, y):
            return x | y

        def contains_compare(col1_param, col2_param):
            """æ£€æŸ¥col1æ˜¯å¦åŒ…å«col2çš„å€¼"""
            return df[col1_param].astype(str).str.contains(df[col2_param].astype(str), regex=False, na=False)

        # æ ¹æ®all_matché€‰æ‹©é€»è¾‘æ“ä½œ
        if all_match:
            mask = True
            op = and_operation
        else:
            mask = False
            op = or_operation

        # éå†æ‰€æœ‰åˆ—å¯¹ï¼Œæ£€æŸ¥æ¯ä¸€å¯¹åˆ—çš„å€¼æ˜¯å¦ç¬¦åˆåŒ…å«å…³ç³»
        for col1, col2 in column_pairs.items():
            condition = contains_compare(col1, col2)
            mask = op(mask, condition)

        # è¿”å›ç¬¦åˆæ¡ä»¶çš„è¡Œ
        return df[mask]

    @staticmethod
    def diff(df1: pd.DataFrame, df2: pd.DataFrame, on_columns: Union[str, List[str]], max_rows: Optional[int] = None) -> pd.DataFrame:
        """
        åŸºäºæŒ‡å®šåˆ—çš„å€¼è¿›è¡Œå”¯ä¸€æ€§åˆ¤æ–­ï¼Œä»df1ä¸­åˆ é™¤df2çš„æ•°æ®ï¼Œä¿ç•™åªå”¯ä¸€å­˜åœ¨äºdf1çš„æ•°æ®

        :param df1: æºDataFrame
        :param df2: è¦ä»ä¸­åˆ é™¤æ•°æ®çš„DataFrame
        :param on_columns: ç”¨äºæ¯”è¾ƒçš„åˆ—åæˆ–åˆ—ååˆ—è¡¨
        :param max_rows: æœ€å¤§è¿”å›è¡Œæ•°, Noneè¡¨ç¤ºä¸é™åˆ¶
        :return: åªåœ¨df1ä¸­å­˜åœ¨è€Œä¸åœ¨df2ä¸­çš„æ•°æ®DataFrame
        """
        # ç¡®ä¿on_columnsæ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(on_columns, str):
            on_columns = [on_columns]

        # æ£€æŸ¥æŒ‡å®šçš„åˆ—æ˜¯å¦éƒ½å­˜åœ¨äºä¸¤ä¸ªDataFrameä¸­
        missing_in_df1 = [col for col in on_columns if col not in df1.columns]
        missing_in_df2 = [col for col in on_columns if col not in df2.columns]

        if missing_in_df1:
            CommonUtil.printLog(f"è­¦å‘Š: åˆ— {missing_in_df1} ä¸å­˜åœ¨äºdf1ä¸­")
        if missing_in_df2:
            CommonUtil.printLog(f"è­¦å‘Š: åˆ— {missing_in_df2} ä¸å­˜åœ¨äºdf2ä¸­")

        result_df = df1.copy()
        if missing_in_df1 or missing_in_df2:
            pass
        else:
            # ä½¿ç”¨mergeå’Œindicatorå‚æ•°æ¥è¯†åˆ«åªåœ¨df1ä¸­å­˜åœ¨çš„è¡Œ
            result_df = result_df.merge(df2[on_columns].copy(), on=on_columns, how='left', indicator=True)
            # ä¿ç•™åªåœ¨df1ä¸­å­˜åœ¨çš„è¡Œ
            result_df = result_df[result_df['_merge'] == 'left_only'].drop('_merge', axis=1)

        result_df = result_df.reset_index(drop=True)
        if max_rows is not None:
            result_df = result_df.head(max_rows)
        return result_df

    @staticmethod
    def convert_dir_excels(src_dir: str, delete_src_excel: bool):
        """
        è½¬æ¢æŒ‡å®šç›®å½•ä¸‹çš„excelæ–‡ä»¶
        :param src_dir: excelæ‰€åœ¨ç›®å½•
        :param delete_src_excel: æ˜¯å¦åˆ é™¤å·²è½¬æ¢çš„excelæºæ–‡ä»¶
        """
        file_list: list = FileUtil.listAllFilePath(src_dir, depth=1)

        for file in file_list:
            full_name, name, ext = FileUtil.getFileName(file)

            if ext not in ['xls', 'xlsx']:
                continue

            converted_csv = f'{src_dir}/{name}.csv'
            CSVUtil.convert_excel(file, converted_csv)

            if delete_src_excel:
                FileUtil.deleteFile(file)

    @staticmethod
    def convert_excel(input_file: str, temp_csv: Optional[str] = None, ignore_exist: bool = True) -> str:
        """
        å¦‚æœè¾“å…¥æ˜¯ Excelï¼Œåˆ™è½¬ä¸ºå¯åˆ†å—è¯»å–çš„ CSV æ–‡ä»¶, å¦åˆ™ç›´æ¥è¿”å›åŸæ–‡ä»¶è·¯å¾„
        :param input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„, æ”¯æŒ: .xlsx  .xls  .csv
        :param temp_csv: excelåå­˜å‚¨çš„csvæ–‡ä»¶è·¯å¾„, è‹¥ä¸ºNone,åˆ™ä½¿ç”¨input_fileåŒç›®å½•ä¸‹, å°†åç¼€æ”¹ä¸ºcsv
        :param ignore_exist: å¿½ç•¥å·²å­˜åœ¨çš„csvæ–‡ä»¶, ç›´æ¥è½¬æ¢å¹¶è¦†ç›–
        :return: è½¬æ¢åçš„æ–‡ä»¶è·¯å¾„
        """
        input_lower = input_file.lower()
        if input_lower.endswith(('.xlsx', '.xls')):
            if CommonUtil.isNoneOrBlank(temp_csv):
                temp_csv = input_file.replace('.xlsx', '.csv').replace('.xls', '.csv')

            temp_csv = FileUtil.recookPath(temp_csv)
            full_name, name, ext = FileUtil.getFileName(input_file)

            if not FileUtil.isFileExist(temp_csv) or ignore_exist:
                CommonUtil.printLog(f"ğŸ”„ æ­£åœ¨å°† Excel è½¬æ¢ä¸º CSV æ–‡ä»¶: {full_name}")
                try:
                    df = pd.read_excel(input_file, dtype=str)
                    CSVUtil.to_csv(df, temp_csv)
                    CommonUtil.printLog(f"âœ… Excel å·²æˆåŠŸè½¬æ¢ä¸º: {temp_csv}")
                except Exception as e:
                    CommonUtil.printLog(f"âŒ Excel è½¬æ¢å¤±è´¥: {e}")
                    raise
            else:
                CommonUtil.printLog(f"âœ… ä½¿ç”¨å·²æœ‰çš„ CSV: {temp_csv}")
            return temp_csv
        else:
            return input_file  # å·²ç»æ˜¯ CSV

    @staticmethod
    def batch_concurrency_process(csv_file: str, output_file: str,
                                  process_row_data: Callable[[pd.Series], None],
                                  col_keyword: str = 'query',
                                  filter_columns_dict: Optional[Dict[str, str]] = None,
                                  chunk_size: int = 1000,
                                  max_concurrent: int = 30,
                                  on_chunk_finished: Callable[[str, pd.DataFrame], None] = None) -> pd.DataFrame:
        """
        ä»csvæ–‡ä»¶ä¸­åˆ†æ‰¹æ¬¡æå–æ•°æ®, æ‰¹æ¬¡å†…éƒ¨å¯¹å„è¡Œæ•°æ®è¿›è¡Œå¹¶å‘å¤„ç†, è¿”å›æ–°ç»“æœ, å¹¶å°†ç»“æœè¦†ç›–å›åŸè¡Œæ•°æ®ä¸­

        :param csv_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„, é€šå¸¸æ˜¯: src.csv
        :param output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„, é€šå¸¸æ˜¯: è‡ªåŠ¨_t.csv
                            è‹¥æ–‡ä»¶å·²å­˜åœ¨, ä¼šè¯»å–å…¶ col_keyword åˆ—ä¿¡æ¯,å»é‡, å¹¶è·³è¿‡ç›¸å…³è¡Œæ•°æ®çš„å¤„ç†
        :param process_row_data: è¡Œæ•°æ®å¤„ç†å‡½æ•°, è¾“å…¥æ˜¯åŸå§‹è¡Œå¯¹è±¡pd.Series, ç›´æ¥åœ¨å…¶ä¸Šä¿®æ”¹å³å¯
        :param col_keyword: åœ¨input/outputæ–‡ä»¶ä¸­éƒ½è¦å­˜åœ¨çš„åˆ—å, ç”¨äºå»é‡, å¤„ç†æ–°è¡Œæ•°æ®æ—¶, è‹¥æ£€æµ‹åˆ°è¯¥åˆ—æ•°æ®å·²æœ‰å¤„ç†è¿‡çš„ç¼“å­˜,åˆ™å®é™…ä½¿ç”¨ç¼“å­˜å€¼
                            filter_columns_dictä¸ºç©ºæ—¶,é»˜è®¤æ˜¯æ£€æµ‹ output_file è¯¥å­˜åœ¨è¯¥åˆ—æ•°æ®æ—¶, å°±è®¤ä¸ºè¿™æ¡æ•°æ® å·²å¤„ç†è¿‡, ä¼šè·³è¿‡
        :param filter_columns_dict: å·²å¤„ç†æ•°æ®çš„è¿‡æ»¤æ¡ä»¶ï¼Œæ ¼å¼ä¸º { åˆ—å: æ­£åˆ™è¡¨è¾¾å¼ }ï¼Œæ”¯æŒå¤šåˆ—è¿‡æ»¤, è¢«è¿‡æ»¤æ¡ä»¶å‘½ä¸­çš„æ•°æ®æ‰è¡¨ç¤ºå·²å¤„ç†è¿‡
        :param chunk_size: æ¯æ¬¡è¯»å–çš„è¡Œæ•°
        :param max_concurrent: æ‰¹æ¬¡å†…éƒ¨æ•°æ®å¤„ç†çš„å¹¶å‘æ•°
        :param on_chunk_finished: æ¯æ‰¹æ¬¡çš„æ•°æ®å¤„ç†å®Œæˆåçš„å›è°ƒå‡½æ•°, è¾“å…¥ä¸º: ç»“æœä¿¡æ¯, å¤„ç†åçš„DataFrame
        """
        result_df = pd.DataFrame()
        # 1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not FileUtil.isFileExist(csv_file):
            CommonUtil.printLog(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
            return result_df

        # 2. åŠ è½½å·²å¤„ç†çš„æ•°æ®ï¼ˆç”¨äºå»é‡ï¼‰
        processed_queries = set()
        if FileUtil.isFileExist(output_file):
            try:
                if CommonUtil.isNoneOrBlank(filter_columns_dict):
                    filter_columns_dict = {}

                if col_keyword not in filter_columns_dict.keys():
                    filter_columns_dict[col_keyword] = r'\S+'  # éç©º
                keys = filter_columns_dict.keys()
                usecols = list(keys)
                df_done = CSVUtil.read_csv(output_file, usecols=usecols)
                df_done = CSVUtil.filter_and_replace(df_done, filter_columns_dict)
                processed_queries = set(df_done[col_keyword].dropna())
                CommonUtil.printLog(f"âœ… æ£€æµ‹åˆ°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œè·³è¿‡ {len(processed_queries)} æ¡å·²å¤„ç†æ•°æ®")
            except Exception as e:
                CommonUtil.printLog(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœå¤±è´¥ï¼Œå°†é‡æ–°å¤„ç†å…¨éƒ¨æ•°æ®: {e}")

        total_processed = 0
        start_time = time.time()

        # 3. åˆ†å—è¯»å–
        try:
            chunk_iter = pd.read_csv(csv_file, chunksize=chunk_size, dtype=str)
        except Exception as e:
            CommonUtil.printLog(f"âŒ æ— æ³•åˆ†å—è¯»å–æ–‡ä»¶ {csv_file}: {e}")
            return result_df

        # 5. å¾ªç¯å¤„ç†æ¯ä¸ª chunk
        for chunk_idx, chunk in enumerate(chunk_iter, start=1):
            if col_keyword not in chunk.columns:
                CommonUtil.printLog(f"âŒ è¾“å…¥æ–‡ä»¶ä¸­ç¼ºå°‘ '{col_keyword}' åˆ—")
                return result_df

            # æ¸…æ´—å¹¶è¿‡æ»¤æ‰ç©ºå€¼å’Œå·²å¤„ç†é¡¹
            queries_to_process = []
            for q in chunk[col_keyword]:
                if pd.isna(q):
                    continue
                q_str = str(q).strip()
                if q_str and q_str not in processed_queries:
                    queries_to_process.append(q_str)

            if not queries_to_process:
                CommonUtil.printLog(f"â­ï¸ æ‰¹æ¬¡ {chunk_idx}: æ— æ–°æ•°æ®ï¼Œè·³è¿‡")
                continue

            CommonUtil.printLog(f"â–¶ å¤„ç†æ‰¹æ¬¡ {chunk_idx} | æ–°å¢å¾…å¤„ç†: {len(queries_to_process)} æ¡")

            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            # with ThreadPoolExecutor ä¼šç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½æ‰§è¡Œå®Œæˆåå†ç»§ç»­æ‰§è¡Œ
            # result_df = pd.DataFrame()
            result_df = chunk
            with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                futures = [executor.submit(process_row_data, chunk.iloc[i]) for i in range(len(chunk))]
                # for future in futures:
                #     try:
                #         res = future.result()
                #         if isinstance(res, pd.Series):
                #             result_df = pd.concat([result_df, res.to_frame().T], ignore_index=True)
                #     except Exception as exc:
                #         CommonUtil.printLog(f"çº¿ç¨‹æ‰§è¡Œå‡ºé”™: {exc}")

            # å¢é‡å†™å…¥ç»“æœ
            if len(result_df) > 0:
                # å†™å…¥æ–‡ä»¶ï¼ˆæ‰¹æ¬¡ä¹‹é—´æ˜¯ä¸²è¡Œçš„ï¼Œä¸éœ€è¦åŠ é”ï¼‰
                write_header = not FileUtil.isFileExist(output_file)
                result_df.to_csv(output_file, mode='a', header=write_header, index=False, encoding='utf-8-sig')

                # æ›´æ–°å·²å¤„ç†é›†åˆ
                processed_queries.update(result_df[col_keyword].astype(str))
                total_processed += len(result_df)
                msg = f"âœ… æ‰¹æ¬¡ {chunk_idx} å®Œæˆ | å†™å…¥: {len(result_df)} æ¡ | ç´¯è®¡æˆåŠŸ: {total_processed} "
                CommonUtil.printLog(msg)
                if on_chunk_finished:
                    on_chunk_finished(msg, result_df)

            # å¯é€‰ï¼šé™ä½è¯·æ±‚å¯†åº¦
            time.sleep(0.5)

        # 6. æ€»ç»“ç»Ÿè®¡
        elapsed = time.time() - start_time
        hours = elapsed / 3600
        CommonUtil.printLog(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼")
        CommonUtil.printLog(f"ğŸ“Š æ€»è€—æ—¶: {elapsed:.1f} ç§’ ({hours:.2f} å°æ—¶)")
        CommonUtil.printLog(f"ğŸ“ˆ æ€»æˆåŠŸæ¡æ•°: {total_processed}")

        CommonUtil.printLog(f"ğŸ“ æœ€ç»ˆç»“æœä¿å­˜è‡³: {output_file}")

    @staticmethod
    def merge_csv_in_dir(src_dir: str, output_csv_name: str = 'merge_result',
                         on_column: str = 'query', rename_cols: Optional[Dict] = None,
                         usecols: List[str] = None, skip_rows: int = 0,
                         reverse_list: bool = False, deduplicate: bool = True,
                         valid_name_pattern: str = '.*.csv',
                         exclude_name_pattern: str = r'^ignore_',
                         src_encoding: str = 'utf-8-sig',
                         save_encoding: str = 'utf-8-sig'
                         ) -> Optional[pd.DataFrame]:
        """
        åˆå¹¶æŒ‡å®šç›®å½•ä¸‹é™¤ 'output_name' ä»¥åŠ 'ignore_' å¼€å¤´çš„ csv æ–‡ä»¶, å¹¶å»é‡, ä¿å­˜ä¸º 'output_name'.csv
        è‹¥å½“å‰ç›®å½•ä¸‹æœ‰excelæ–‡ä»¶,è¯·è‡ªè¡Œè°ƒç”¨ convert_dir_excel() è½¬æ¢æˆcsvæ–‡ä»¶å†è°ƒç”¨æœ¬æ–¹æ³•è¿›è¡Œåˆå¹¶
        è¦è¯»å–å’Œä¿å­˜çš„åˆ—åä¸ºç”± 'usecols' å®šä¹‰, è¯·ç¡®ä¿è¿™äº›åˆ—åå­˜åœ¨, è‹¥ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºä¸€åˆ—ç©ºç™½åˆ—
        æœ€åä¼šæ–°å¢ä¸€åˆ—: 'result_src' ç”¨ä»¥è®°å½•å½“å‰æ•°æ®æ¥æºäºå“ªä»½æ–‡æ¡£

        :param src_dir: æºcsv/xls/xlsx æ–‡ä»¶æ‰€åœ¨ç›®å½•, è¾“å‡ºæ–‡ä»¶ä¹Ÿä¼šå­˜å‚¨åœ¨è¿™ä¸ªç›®å½•ä¸­, æ¯”å¦‚è„šæœ¬æ‰€åœ¨ç›®å½•: os.path.dirname(os.path.abspath(__file__))
        :param output_csv_name: æœ€ç»ˆåˆå¹¶ç”Ÿæˆçš„csvæ–‡ä»¶å(ä¸åŒ…å« .csv åç¼€), ä¼ ç©ºè¡¨ç¤ºä¸ä¿å­˜
        :param reverse_list: è·å–åˆ°çš„csvæ–‡ä»¶æ˜¯æŒ‰åç§°è‡ªç„¶æ’åºçš„, æ˜¯å¦è¦å€’åº
        :param on_column: åˆå¹¶å’Œå»é‡æ•°æ®æ—¶çš„åˆ—ä¾æ®, éç©º, è‹¥ rename_cols éç©º, åˆ™è¦æ±‚ on_columnå­˜åœ¨äºé‡å‘½ååçš„åˆ—åä¸­
        :param rename_cols: è¯»å–csvå, å¯¹åˆ—åè¿›è¡Œé‡å‘½å, æ ¼å¼ä¸º: { åŸåˆ—å: æ–°åˆ—å }
        :param usecols: è¯»å–csvæ–‡ä»¶æ—¶è¦è¯»å–çš„åˆ—æ•°æ®, Noneè¡¨ç¤ºå…¨éƒ¨è¯»å–
        :param skip_rows: è¯»å–csvæ–‡ä»¶æ—¶, è¦è·³è¿‡çš„è¡¨å¤´è¡Œæ•°, æ³¨æ„è‹¥è·³è¿‡åè¯»å–åˆ°çš„dfä¸å­˜åœ¨ on_column åˆ—, åˆ™ä¼šå–æ¶ˆè·³è¿‡,é‡æ–°è¯»å–æ–‡ä»¶
        :param deduplicate: åˆå¹¶åçš„æ•°æ®æ˜¯å¦è¦å»é‡
        :param valid_name_pattern: è¦åˆå¹¶çš„csvæ–‡ä»¶å(åŒ…å«åç¼€)è¦æ»¡è¶³çš„æ­£åˆ™è¡¨è¾¾å¼
        :param exclude_name_pattern: è¦å‰”é™¤çš„csvæ–‡ä»¶åæ­£åˆ™è¡¨è¾¾å¼
        :param src_encoding: åŸcsvæ‰€ç”¨ç¼–ç , ç”¨äºè¯»å–
        :param save_encoding: csvåˆå¹¶åä¿å­˜æ—¶æ‰€ç”¨çš„ç¼–ç 

        æ¯”å¦‚å¯¹äºå¾®ä¿¡å¯¹è´¦å•excelæ–‡ä»¶, ä¼šå…ˆè½¬åŒ–ä¸ºcsv, ç„¶ååˆå¹¶csv(åŸºäºæ—¶é—´å»é‡)
        å¾®ä¿¡å¯¹è´¦å•å‰16è¡Œä¸ºç»Ÿè®¡ä¿¡æ¯è¡¨å¤´, éœ€è¦è·³è¿‡
        å¾®ä¿¡å¯¹è´¦å•çš„è¯¦æƒ…åˆ—åä¸º:
        äº¤æ˜“æ—¶é—´,äº¤æ˜“ç±»å‹,äº¤æ˜“å¯¹æ–¹,å•†å“,æ”¶/æ”¯,é‡‘é¢(å…ƒ),æ”¯ä»˜æ–¹å¼,å½“å‰çŠ¶æ€,äº¤æ˜“å•å·,å•†æˆ·å•å·,å¤‡æ³¨
        """
        if CommonUtil.isNoneOrBlank(output_csv_name):
            output_csv = ''
        else:
            output_csv: str = f'{src_dir}/{output_csv_name}.csv'  # æœ€ç»ˆç”Ÿæˆçš„å…¨é‡csvæ–‡ä»¶(å·²å»é‡)
        file_list: list = FileUtil.listAllFilePath(src_dir, depth=1)

        valid_csv_list = []
        for file in file_list:
            full_name, name, ext = FileUtil.getFileName(file)

            if output_csv_name != name and ext == 'csv':
                if re.search(valid_name_pattern, full_name) and not re.search(exclude_name_pattern, full_name):
                    valid_csv_list.append(file)

        valid_csv_list = sorted(valid_csv_list, reverse=reverse_list)
        CommonUtil.printLog(f'å¾…åˆå¹¶çš„csvæ–‡ä»¶åˆ—è¡¨ä¸º: {[FileUtil.getFileName(x)[0] for x in valid_csv_list]}')
        df = None
        for file in valid_csv_list:
            full_name, name, ext = FileUtil.getFileName(file)
            df_file = CSVUtil.read_csv(file, skip_rows=skip_rows, encoding=src_encoding)
            if not CommonUtil.isNoneOrBlank(rename_cols):
                df_file = df_file.rename(columns=rename_cols)

            if on_column not in df_file.columns:
                df_file = CSVUtil.read_csv(file, encoding=src_encoding)

                if not CommonUtil.isNoneOrBlank(rename_cols):
                    df_file = df_file.rename(columns=rename_cols)

            df_file = CSVUtil.reorder_cols(df_file, usecols)
            df_file['result_src'] = full_name  # æ•°æ®æ¥æº
            if df is None:
                df = df_file
                continue
            df = pd.concat([df, df_file], ignore_index=True)  # ç¡®ä¿æ•°æ®å®Œæ•´,ä¸åŸå§‹å€¼ä¿æŒä¸€è‡´
            # df = CSVUtil.merge_dataframe(df, df_file, on_column=on_column, deduplicate=deduplicate)

        if df is None:
            print('merge_csv_files fail: df is None')
        else:
            if deduplicate:
                df = CSVUtil.deduplicate(df, on_column)  # å»é‡
            if not CommonUtil.isNoneOrBlank(output_csv):
                CSVUtil.to_csv(df, output_csv, encoding=save_encoding)
            print(f'merge_csv_files success: {output_csv_name}.csv saved, total rows: {len(df)}')
        return df

    @staticmethod
    def statistics_multi_col(df: pd.DataFrame, cols: List[str],
                             output_dir: str = None,
                             generate_img: bool = True,
                             show_img: bool = False,
                             merged_img_name: Optional[str] = 'merged_image_distribution.png',
                             round_digits: int = 1,
                             min_value: Union[float, int, str] = 1e-5,
                             nan_replace_value: Union[float, int, str] = 1e-6,
                             custom_index: List[str] = None) -> pd.DataFrame:
        """
        åŒæ—¶è®¡ç®—å¤šåˆ—çš„ç»Ÿè®¡æ•°æ®å¹¶ç»˜åˆ¶å„åˆ—çš„æ­£æ€åˆ†å¸ƒå›¾, ç„¶åå°†å›¾åˆå¹¶æˆä¸€å¼ , ä¿å­˜åˆ° output_dir/merged_image_distribution.png
        :param df: å¾…ç»Ÿè®¡çš„DataFrame
        :param cols: å¾…ç»Ÿè®¡çš„åˆ—ååˆ—è¡¨
        :param output_dir: è¾“å‡ºç›®å½•, ç”¨äºå­˜å‚¨å›¾ç‰‡, è‹¥ä¼ ç©º, åˆ™ä¸ä¿å­˜å›¾ç‰‡
        :param generate_img: æ˜¯å¦è¦ç»˜åˆ¶æ­£åˆ™åˆ†å¸ƒå›¾
        :param show_img: æ‰€æœ‰æ­£æ€åˆ†å¸ƒå›¾ç»˜åˆ¶å®Œæˆå,æ˜¯å¦è¦æ˜¾ç¤ºåˆå¹¶ç»“æœå›¾ é»˜è®¤False,
        :param merged_img_name: åˆå¹¶æ‰€æœ‰æ­£æ€åˆ†å¸ƒå›¾åç”Ÿæˆçš„åˆå¹¶å›¾ç‰‡åç§°(å¸¦åç¼€), éç©ºæ—¶æ‰ä¼šåˆå¹¶å›¾ç‰‡
        :param round_digits: æå¤§å€¼/æå°å€¼/ä¸­ä½æ•°/å¹³å‡å€¼/æ ‡å‡†å·® è¿™å‡ ä¸ªfloatæ•°æ®å››èˆäº”å…¥è¦ä¿ç•™å‡ ä½å°æ•°, é»˜è®¤1ä½
        :param min_value: ç»Ÿè®¡åˆ—æ•°æ®æ—¶, å…è®¸çš„æœ€å°å€¼, åªç»Ÿè®¡ >=min_value çš„æ•°æ®éƒ¨åˆ†
        :param nan_replace_value: nanæ•°æ®æ›¿æ¢ä¸ºæŒ‡å®šå€¼
        :param custom_index: è‡ªå®šä¹‰åˆ—å, è‹¥ä¸ºç©º, åˆ™ä½¿ç”¨ cols ä½œä¸ºæœ€ç»ˆè¿”å›çš„dataframe indexå, å…è®¸éƒ¨åˆ†å…ƒç´ ä¸ºç©º, ä¼šä½¿ç”¨ cols æ›¿ä»£
        :return å³°ä¼šå„åˆ—çš„ç»Ÿè®¡æ•°æ®æ±‡æ€»è¡¨, åŒ…å«: 'æ ·æœ¬æ•°', 'æå¤§å€¼', 'æå°å€¼', 'ä¸­ä½æ•°', 'å¹³å‡å€¼', 'æ ‡å‡†å·®', 'æ­£æ€åˆ†å¸ƒå›¾'
        """
        index = []
        sample_list, max_list, min_list, median_list, mean_list, std_list = [], [], [], [], [], []
        img_list = []  # æ­£æ€åˆ†å¸ƒå›¾çš„ä¿å­˜è·¯å¾„

        custom_index_size = 0 if CommonUtil.isNoneOrBlank(custom_index) else len(custom_index)
        for i in range(len(cols)):
            col = cols[i]
            custom_col_name = col
            if custom_index_size > 0 and i < custom_index_size:
                custom_col_name = custom_index[i]
            index.append(custom_col_name)

            # æ­¤å¤„ä¸æ˜¾ç¤º,é¿å…é˜»å¡åç»­æµç¨‹
            col_dict = CSVUtil.statistics_col(df, col, output_dir=output_dir,
                                              generate_img=generate_img, show_img=False,
                                              min_value=min_value, custom_col_name=custom_col_name)
            sample_list.append(col_dict['sample_size'])
            max_list.append(col_dict['max'])
            min_list.append(col_dict['min'])
            median_list.append(col_dict['median'])
            mean_list.append(col_dict['mean'])
            std_list.append(col_dict['std'])
            img_list.append(col_dict['img_path'])

        # åˆ›å»º DataFrame
        data = {
            'sample_size': sample_list,
            'max': max_list,
            'min': min_list,
            'median': median_list,
            'mean': mean_list,
            'std': std_list,
            'æ­£æ€åˆ†å¸ƒå›¾': img_list
        }
        df = pd.DataFrame(data, index=index)

        # è®¾ç½®åˆ—åï¼ˆå¦‚æœéœ€è¦ï¼‰
        df.columns = ['æ ·æœ¬æ•°', 'æå¤§å€¼', 'æå°å€¼', 'ä¸­ä½æ•°', 'å¹³å‡å€¼', 'æ ‡å‡†å·®', 'æ­£æ€åˆ†å¸ƒå›¾']

        # æ ·æœ¬æ•°åˆ—è½¬ä¸ºintå‹
        df['æ ·æœ¬æ•°'] = df['æ ·æœ¬æ•°'].fillna(nan_replace_value).astype(int)

        # å°†æå¤§å€¼/æå°å€¼/ä¸­ä½æ•°/å¹³å‡å€¼/æ ‡å‡†å·® floatæ•°æ®ä¿ç•™1ä½å°æ•°ï¼ˆå…ˆå¡«å…… NaN å€¼ä¸º 0ï¼‰
        df[['æå¤§å€¼', 'æå°å€¼', 'ä¸­ä½æ•°', 'å¹³å‡å€¼', 'æ ‡å‡†å·®']] = (df[['æå¤§å€¼', 'æå°å€¼', 'ä¸­ä½æ•°', 'å¹³å‡å€¼', 'æ ‡å‡†å·®']]
                                                                  .fillna(nan_replace_value)
                                                                  .round(round_digits))

        # å°†æ‰€æœ‰æ­£æ€åˆ†å¸ƒå›¾åˆå¹¶ä¸ºä¸€å¼ 
        # è¿‡æ»¤ img_list éç©ºçš„æ•°æ®
        img_list = [x for x in img_list if x]
        img_size = len(img_list)
        if img_size >= 2 and not CommonUtil.isNoneOrBlank(merged_img_name):
            mod = img_size % 2
            row_size = img_size // 2 + mod  # 2å¼ å›¾ä¸€è¡Œ
            from util.ImageUtil import ImageUtil
            merge_image = ImageUtil.merge_images(img_list, rows=row_size)
            image_path = FileUtil.recookPath(f'{output_dir}/{merged_img_name}')
            ImageUtil.save_img(image_path, merge_image)
            CommonUtil.printLog(f'{cols}çš„æ­£æ€åˆ†å¸ƒå›¾åˆå¹¶æˆåŠŸ: {image_path}')
            # CommonUtil.printLog(f'{cols}çš„æå¤§å€¼æå°å€¼ç­‰ç»Ÿè®¡ä¿¡æ¯å¦‚ä¸‹: {df}')
            if generate_img and show_img:
                ImageUtil(merge_image).show()
        return df

    @staticmethod
    def statistics_col(df: pd.DataFrame, col: str,
                       x_label_name: str = 'è€—æ—¶',
                       output_dir: str = None,
                       generate_img: bool = True,
                       show_img: bool = True,
                       min_value: float = 1e-5,
                       custom_col_name: str = '') -> Dict[str, Union[float, int, str, None]]:
        """
        ç»Ÿè®¡æŒ‡å®šåˆ—çš„çš„å„æŒ‡æ ‡ä¸»å¥å¹¶ç»˜åˆ¶æ­£æ€åˆ†å¸ƒå›¾ (åŒYè½´è®¾è®¡:é¢‘ç‡å æ¯”% + æ¦‚ç‡å¯†åº¦)
        :param df: å¾…ç»Ÿè®¡çš„DataFrame
        :param col: å¾…ç»Ÿè®¡çš„åˆ—å
        :param x_label_name: ç»˜åˆ¶æ­£æ€åˆ†å¸ƒå›¾æ—¶, xè½´çš„åç§°
        :param output_dir: è¾“å‡ºç›®å½•, ç”¨äºå­˜å‚¨å›¾ç‰‡, è‹¥ä¼ ç©º, åˆ™ä¸ä¿å­˜å›¾ç‰‡
        :param generate_img: æ˜¯å¦è¦ç»˜åˆ¶æ­£åˆ™åˆ†å¸ƒå›¾
        :param show_img: æ­£æ€åˆ†å¸ƒå›¾ç»˜åˆ¶å®Œæˆå,æ˜¯å¦è¦ç›´æ¥æ˜¾ç¤º
        :param min_value: ç»Ÿè®¡åˆ—æ•°æ®æ—¶, å…è®¸çš„æœ€å°å€¼, åªç»Ÿè®¡ >=min_value çš„æ•°æ®éƒ¨åˆ†
        :param custom_col_name: è‡ªå®šä¹‰åˆ—å, è‹¥ä¸ºç©º, åˆ™ä½¿ç”¨ col ä½œä¸ºæ­£æ€åˆ†å¸ƒå›¾çš„æ ‡é¢˜å
        :return dict: ç»Ÿè®¡æ•°æ®åŠæ­£æ€åˆ†å¸ƒå›¾ä¿å­˜åœ°å€
                key: max/min/median/mean/std/sample_size/img_path
                å«ä¹‰: æå¤§å€¼/æå°å€¼/ä¸­ä½æ•°/å¹³å‡å€¼/æ ‡å‡†å·®/æ ·æœ¬æ•°/æ­£æ€åˆ†å¸ƒå›¾ç‰‡ä¿å­˜åœ°å€
        """
        result_keys = ['max', 'min', 'median', 'mean', 'std', 'sample_size', 'img_path']
        result_dict: Dict[str, Union[float, int, str, None]] = {item: None for item in result_keys}
        custom_col_name = col if CommonUtil.isNoneOrBlank(custom_col_name) else custom_col_name

        # é¿å… SettingWithCopyWarning
        df = df.copy()

        # å…ˆè½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œå¤„ç†å­—ç¬¦ä¸²å’Œç©ºå€¼
        df[col] = pd.to_numeric(df[col], errors='coerce')
        df = df.dropna(subset=[col])
        df = df[df[col] >= min_value]

        if not df.empty:
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            max_cost = df[col].max()  # æå¤§å€¼
            min_cost = df[col].min()  # æå°å€¼
            median_cost = df[col].median()  # ä¸­ä½æ•°
            mean_cost = df[col].mean()  # å¹³å‡å€¼
            std_cost = df[col].std()  # æ ‡å‡†å·®

            CommonUtil.printLog(f'ğŸ“Š {custom_col_name} ç»Ÿè®¡æ•°æ®:')
            CommonUtil.printLog(f'   æå¤§å€¼: {max_cost:.2f} ms')
            CommonUtil.printLog(f'   æå°å€¼: {min_cost:.2f} ms')
            CommonUtil.printLog(f'   ä¸­ä½æ•°: {median_cost:.2f} ms')
            CommonUtil.printLog(f'   å¹³å‡å€¼: {mean_cost:.2f} ms')
            CommonUtil.printLog(f'   æ ‡å‡†å·®: {std_cost:.2f} ms')
            CommonUtil.printLog(f'   æ ·æœ¬æ•°: {len(df)}')

            result_dict['max'] = max_cost  # æå¤§å€¼
            result_dict['min'] = min_cost  # æå°å€¼
            result_dict['median'] = median_cost  # ä¸­ä½æ•°
            result_dict['mean'] = mean_cost  # å¹³å‡å€¼
            result_dict['std'] = std_cost  # æ ‡å‡†å·®
            result_dict['sample_size'] = len(df)  # æ ·æœ¬æ•°

            if generate_img:  # ç»˜åˆ¶æ­£æ€åˆ†å¸ƒå›¾
                import matplotlib.pyplot as plt
                import matplotlib
                import matplotlib.ticker as mticker
                from scipy import stats

                matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']  # æ”¯æŒä¸­æ–‡
                matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

                fig, ax1 = plt.subplots(1, 1, figsize=(12, 7))
                ax2 = ax1.twinx()

                # æå–æ•°æ®åˆ—è¡¨
                data_list = df[col].tolist()
                data_count = len(data_list)
                data_range = max_cost - min_cost

                # åŠ¨æ€è®¡ç®— bins
                if data_range == 0:
                    # å¦‚æœæ‰€æœ‰æ•°æ®ç‚¹éƒ½ç›¸åŒï¼Œåˆ›å»ºä¸€ä¸ªä»¥è¯¥å€¼ä¸ºä¸­å¿ƒçš„å•ä¸ªbin
                    single_value = min_cost
                    bins = [single_value - 5, single_value + 5]
                else:
                    # åŠ¨æ€binè®¡ç®—é€»è¾‘
                    num_bins = max(1, min(15, int(np.sqrt(data_count))))
                    bin_width = max(1, np.ceil(data_range / num_bins))
                    start_bin = np.floor(min_cost / bin_width) * bin_width
                    end_bin = np.ceil(max_cost / bin_width) * bin_width + bin_width
                    bins = np.arange(start_bin, end_bin, bin_width)

                # ä½¿ç”¨ weights å‚æ•°å°†ç›´æ–¹å›¾è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                weights = np.ones_like(data_list) * 100. / data_count
                _, _, hist_patches = ax1.hist(data_list, bins=bins, weights=weights, alpha=0.6,
                                              color='#1f77b4', edgecolor='black')

                # æ”¶é›†å›¾ä¾‹å…ƒç´ 
                handles = [hist_patches[0]]
                labels = ['é¢‘ç‡åˆ†å¸ƒç›´æ–¹å›¾']

                # ä»…åœ¨æ ‡å‡†å·®>0æ—¶ç»˜åˆ¶æ­£æ€æ›²çº¿
                if std_cost > 0:
                    x_curve = np.linspace(min_cost - std_cost, max_cost + std_cost, 200)
                    p_curve = stats.norm.pdf(x_curve, mean_cost, std_cost)
                    curve_line, = ax2.plot(x_curve, p_curve, 'r-', linewidth=2.5)
                    handles.append(curve_line)
                    labels.append('æ­£æ€åˆ†å¸ƒæ›²çº¿')
                else:
                    # å¦‚æœä¸ç”»æ›²çº¿ï¼Œéšè—å³ä¾§Yè½´
                    ax2.get_yaxis().set_visible(False)

                # ç»˜åˆ¶å¹³å‡å€¼å’Œä¸­ä½æ•°è™šçº¿
                mean_line = ax1.axvline(mean_cost, color='red', linestyle='dashed', linewidth=1.5)
                median_line = ax1.axvline(median_cost, color='green', linestyle='dashed', linewidth=1.5)

                handles.extend([mean_line, median_line])
                labels.extend([
                    f'å¹³å‡å€¼: {mean_cost:.2f}',
                    f'ä¸­ä½æ•°: {median_cost:.2f}'
                ])

                # è®¾ç½®Xè½´åˆ»åº¦
                ax1.set_xticks(bins)
                ax1.xaxis.set_major_formatter(mticker.FormatStrFormatter('%d'))
                plt.setp(ax1.get_xticklabels(), rotation=30, ha="right")

                # è®¾ç½®Yè½´ä¸ºç™¾åˆ†æ¯”æ ¼å¼
                ax1.yaxis.set_major_formatter(mticker.PercentFormatter())

                # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
                ax1.set_xlabel(x_label_name, fontsize=11, fontweight='bold')
                ax1.set_ylabel('é¢‘ç‡å æ¯” (%)', color='#1f77b4', fontweight='bold', fontsize=11)
                ax2.set_ylabel('æ¦‚ç‡å¯†åº¦', color='red', fontweight='bold', fontsize=11)
                ax1.tick_params(axis='y', labelcolor='#1f77b4', labelsize=10)
                ax2.tick_params(axis='y', labelcolor='red', labelsize=10)
                ax1.set_title(f'{custom_col_name}-æ­£æ€åˆ†å¸ƒ', fontsize=16, weight='bold')

                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†
                stats_text = (
                    f"ç»Ÿè®¡ä¿¡æ¯\n"
                    f"----------------\n"
                    f"æ•°æ®ç‚¹æ•°: {data_count}\n"
                    f"æ ‡å‡†å·®: {std_cost:.2f} ms\n"
                    f"æœ€å°å€¼: {min_cost:.2f} ms\n"
                    f"æœ€å¤§å€¼: {max_cost:.2f} ms"
                )
                ax1.annotate(stats_text, xy=(0.85, 0.97), xycoords='axes fraction', ha='left', va='top',
                             bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor='gray'),
                             fontsize=10)

                # åˆ›å»ºå›¾ä¾‹
                ax1.legend(handles, labels, loc='upper left', fontsize=9)

                # è®¾ç½®ç½‘æ ¼
                ax1.grid(True, linestyle='--', alpha=0.6)
                ax2.grid(False)
                plt.tight_layout()

                # ä¿å­˜å›¾ç‰‡
                if not CommonUtil.isNoneOrBlank(output_dir):
                    plot_file = f'{output_dir}/distribution_{custom_col_name}.png'
                    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
                    CommonUtil.printLog(f'ğŸ“ˆ {custom_col_name}åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_file}')
                    result_dict['img_path'] = plot_file

                # æ˜¾ç¤ºå›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
                if show_img:
                    plt.show()
        else:
            CommonUtil.printLog(f'âš ï¸ æ²¡æœ‰æ‰¾åˆ° {custom_col_name} >= {min_value} çš„æ•°æ®')
        return result_dict
